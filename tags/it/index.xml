<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IT on seandavi(s12)</title>
    <link>/tags/it/</link>
    <description>Recent content in IT on seandavi(s12)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 19 Jul 2018 14:35:47 -0400</lastBuildDate>
    
	<atom:link href="/tags/it/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Infrastructure-as-Code: Building the Bioconductor Conference AMI With Packer</title>
      <link>/2018/07/infrastructure-as-code-building-the-bioconductor-conference-ami-with-packer/</link>
      <pubDate>Thu, 19 Jul 2018 14:35:47 -0400</pubDate>
      
      <guid>/2018/07/infrastructure-as-code-building-the-bioconductor-conference-ami-with-packer/</guid>
      <description>One of the main features of the annual Bioconductor Conference is the proportion of time spent working with code in the form of workshops. To support these workshops, we ask workshop presenters to supply Rmarkdown materials which we collate into workshop materials. Using literate programming approaches like Rmarkdown ensures that the workflows are self-consistent and work as expected.
In addition to the Rmarkdown workshop materials, we also need a consistent computing environment that can support reasonably large computation, provide high-performance network and file system access, and is essentially unlimited in scale (we expect to have &amp;gt;150 participants, each with his/her own machine).</description>
    </item>
    
    <item>
      <title>Create a basic Apache Spark cluster in the cloud (in 5 minutes)</title>
      <link>/2018/02/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</guid>
      <description>Apache Spark in a few words Apache Spark is a software and data science platform that is purpose-built for large- to massive-scale data processing. Spark supports processing of data in batch mode (run as a pipeline) or in interactive mode using command-line programming style or in popular notebook style of coding. While scala is the native language for Spark, language bindings exist for python, R, and Java as well.</description>
    </item>
    
  </channel>
</rss>