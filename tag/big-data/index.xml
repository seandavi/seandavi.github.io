<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Big Data | Academic</title>
    <link>/tag/big-data/</link>
      <atom:link href="/tag/big-data/index.xml" rel="self" type="application/rss+xml" />
    <description>Big Data</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 02 Feb 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Big Data</title>
      <link>/tag/big-data/</link>
    </image>
    
    <item>
      <title>Create a basic Apache Spark cluster in the cloud (in 5 minutes)</title>
      <link>/post/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/post/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</guid>
      <description>


&lt;div id=&#34;apache-spark-in-a-few-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apache Spark in a few words&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; is a software and data science platform that is
purpose-built for large- to massive-scale data processing. Spark
supports processing of data in batch mode (run as a pipeline) or in
interactive mode using command-line programming style or in popular
notebook style of coding. While &lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;scala&lt;/a&gt; is the native language for
Spark, language bindings exist for python, R, and Java as well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org&#34; &gt;
&lt;img
src=&#34;https://www.onlinebooksreview.com/uploads/blog_images/2017/11/27_file.png&#34;
 style=&#34;width:300px;float:left;padding-right:10px&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spark is built around an underlying data abstraction called Resilient
Distributed Dataset, or RDD. The RDD represents an immutable,
distributed, and partitioned collection of elements that can be
operated on in parallel. The collection of elements in the RDD need
not fit into memory, though the performance is maximal when the RDD
fits into the Spark “cluster” memory.&lt;/p&gt;
&lt;p&gt;Spark is capable of processing
data &lt;a href=&#34;https://spark.apache.org/faq.html&#34;&gt;at very large scales&lt;/a&gt;. That
said, code for Spark need not be written on a large cluster. Spark can
be deployed on a laptop as well, facilitating code development and
testing at a small scale.&lt;/p&gt;
&lt;p&gt;I am not going to delve into working with Spark just yet. Rather, I am
starting with a quick walkthrough of creating a Spark cluster on the
Amazon Web Services Elastic Map Reduce
service &lt;a href=&#34;https://aws.amazon.com/emr/&#34;&gt;AWS EMR&lt;/a&gt;. This is not a
recommendation of AWS over other potential providers and choices in
the following workflow are &lt;em&gt;not&lt;/em&gt; meant as best practices. This is just
a documentation of the process with a little text.&lt;/p&gt;
&lt;style&gt;
img {
  width: 100%
    }
.alert {
  background-color: #fcc2c2;
  border-radius: 10px;
  padding: 5px;
  padding-left: 20px;
  padding-right: 20px;
  }
&lt;/style&gt;
&lt;div class=&#34;alert&#34;&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This blog post creates resources on a commercial cloud which
will continue to cost money until they are terminated. In
order to keep from getting charged for unused resources, be sure to
clean up by terminating the resources once you are done.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The main prerequisite is an AWS account.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Walk through creating an Apache Spark cluster on AWS using the EMR
service.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-go&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s go&lt;/h2&gt;
&lt;p&gt;Login to your AWS &lt;a href=&#34;https://console.aws.amazon.com&#34;&gt;console&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After logging in, you should see a window with a lot of AWS services
listed. At the top left, choose the “Services” button and type “EMR”
into the search box. Then, choose EMR.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/chooseEMRService.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the next screen, choose “Create Cluster” by clicking the blue
button. Just a note that the “AWS Glue Catalog” that is featured
prominently in a couple of places in the configuration is a separatemarkdow
service from AWS, &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html&#34;&gt;detailed here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/createcluster.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On this screen, choose an arbitrary name for your cluster. You can
choose no logging for now, or specify a logging s3 bucket/path if you
like. Change the software configuration to Spark as shown (version
numbers may differ–that is OK).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/configuration2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next step is to configure the hardware that will comprise the
cluster. Choosing the appropriate size and number of machines while
balancing costs is an art form that I have not mastered. However, you
will likely want at least a master and one worker (specify 2 or
more). Starting with the defaults for “experimentation” is probably
not bad. Roughly speaking, you’ll want enough memory on your cluster
to support keeping your datasets in memory for maximum performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/hardwareconfigbasic.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike many commercially-offered online services, AWS EMR is not, by
default, configured for “open” access. To gain access to the cluster,
you will need to provide an SSH key &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair&#34;&gt;see her for how to generate an
ssh key on AWS&lt;/a&gt; that
you will later use to enable access to the Spark notebook and
web-based user interface for monitoring. Assuming that you have an SSH
key created, choose that key in the dropdown as pictured below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/sshkey.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Click the “Create Cluster” at the bottom right of the screen. You
should then be presented with a page that shows the cluster
details.&lt;/p&gt;
&lt;p&gt;On AWS, cluster creation takes several minutes to up to 30 minutes. My
only other experience with cloud Spark-as-a-service is on Google Cloud
Platform which has a much faster startup time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/startingup.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After a few minutes, you will have a running Apache Spark cluster that
you can begin to experiment with. That will need to wait for another
post, but to gain access to the cluster, including a Zeppelin notebook
(quite similar to Jupyter), click the “Enable Web Connection” and
follow the instructions (a little involved, including establishing a
proxy connection and installing a proxy plugin for your browser).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/enablesshconn.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cleanup-very-important&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleanup: VERY IMPORTANT&lt;/h2&gt;
&lt;div class=&#34;alert&#34;&gt;
&lt;p&gt;After you have created the Spark cluster, it &lt;strong&gt;costs money&lt;/strong&gt; until you
destroy it. &lt;em&gt;Please do not forget&lt;/em&gt; to click “Terminate” and then check
back to &lt;em&gt;make doubly sure&lt;/em&gt; that the cluster is terminated.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
