<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker | Academic</title>
    <link>/tag/docker/</link>
      <atom:link href="/tag/docker/index.xml" rel="self" type="application/rss+xml" />
    <description>docker</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 05 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>docker</title>
      <link>/tag/docker/</link>
    </image>
    
    <item>
      <title>Build and deploy an NCBI GEO metadata fetch API</title>
      <link>/post/cloud-run-notes/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/cloud-run-notes/</guid>
      <description>&lt;p&gt;In this post, I want to demonstrate building a simple web API converts
an NCBI &lt;a href=&#34;https://ncbi.nlm.nih.gov/geo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GEO&lt;/a&gt; accession and associated metadata to &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt; format, and
then deploy that application as a web service on Google Cloud Platform
&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I hear from GEOquery users that sometimes they just want to get the
metadata for one or more accessions rather than getting the entire GEO
record. My &lt;a href=&#34;https://github.com/omicidx/omicidx-parsers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omicidx-parser&lt;/a&gt; library has functionality to do this
conversion. We will leverage this functionality to build a small web
application that we can deploy using &lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; approach to stand up
an API that returns a &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt; conversion of GEO metadata.&lt;/p&gt;
&lt;h2 id=&#34;tooling&#34;&gt;Tooling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Google cloud platform, specifically &lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;python programming language&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; library for web API development&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[docker] containers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;git and github&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Code: &lt;a href=&#34;https://github.com/seandavi/blog-code/&#34;&gt;https://github.com/seandavi/blog-code/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-will-i-learn-and-do&#34;&gt;What will I learn and do?&lt;/h2&gt;
&lt;p&gt;In this post, we will learn the basics of &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; through
examples. Additionally, we will learn a bit about web app development
using the &lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; python web framework.&lt;/p&gt;
&lt;p&gt;At the end of this post, you will have a containerized web application
running locally that can take a GEO accession and return a &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt;
version of the metadata.&lt;/p&gt;
&lt;p&gt;If you have a Google Cloud Platform account that allows you to create
projects, you should be able to run your application in Google Cloud
Run.&lt;/p&gt;
&lt;h2 id=&#34;what-is-cloud-run&#34;&gt;What is Cloud Run?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; is a fully managed compute platform that automatically
scales your stateless containers. In other words, write a web
application, place it in a &lt;a href=&#34;https://docker.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker container&lt;/a&gt;, and then deploy to the
cloud. Cloud Run is one of a family of technologies referred to as
&lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; it abstracts away all infrastructure management. Cloud
Run is built upon an open standard, &lt;a href=&#34;https://knative.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;, enabling the
portability of your applications.&lt;/p&gt;
&lt;h2 id=&#34;build-your-app&#34;&gt;Build your app&lt;/h2&gt;
&lt;p&gt;Sorry, but I&amp;rsquo;m going to let the code do most of the talking here. In
short, Google Cloud Run supports any web development language or
framework that can be deployed as a docker container.&lt;/p&gt;
&lt;p&gt;Here, I will use python and adopt &lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; a framework for building
performant REST APIs.&lt;/p&gt;
&lt;p&gt;The GEO parsing will leverage the &lt;a href=&#34;https://github.com/omicidx/omicidx-parsers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omicidx-parser&lt;/a&gt; library.&lt;/p&gt;
&lt;h2 id=&#34;containers-are-front-and-center&#34;&gt;Containers are front-and-center&lt;/h2&gt;
&lt;p&gt;Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; creates a &amp;ldquo;service&amp;rdquo; that accepts http requests
and returns responses. The requests and responses are not predefined
by Cloud Run, allowing any approach that your chosen programming
language supports. Once your web application is written and tested
locally, create a &lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerfile&lt;/a&gt; that generates a containerized version
of it. The generated docker container is what you will pass along to
&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; as the application. &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; will then create a
&amp;ldquo;service&amp;rdquo; by setting up your docker container to run &amp;ldquo;on demand&amp;rdquo; when
a web request is directed to it.&lt;/p&gt;
&lt;p&gt;Any programming dependencies, operating system or system dependencies,
or necessary software are simply included in the &lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerfile&lt;/a&gt;
description to be incorporated into the container. Local testing is
straightforward by simply running the docker container locally.&lt;/p&gt;
&lt;h2 id=&#34;example-application-and-deployment&#34;&gt;Example application and deployment&lt;/h2&gt;
&lt;p&gt;As of today, I am using &lt;a href=&#34;https://python-poetry.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poetry&lt;/a&gt; for python dependency management
(think pip and virtualenv replacement) and package building. To get
started with poetry, install:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py \
  | python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;ll leave the poetry parts of things out for now, but you will see
some poetry command lines to build the cloud run environment.&lt;/p&gt;
&lt;p&gt;Next, checkout the &lt;a href=&#34;https://github.com/seandavi/blog-code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example repo&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/seandavi/blog-code
cd blog-code/geo-metadata-cloud-run/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To set up a local development environment and then test the app
locally, I use a couple of poetry commands (see poetry docs for what
these do, specifically).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install
poetry run uvicorn app.main:app
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, navigate to http://localhost:8000/docs. There, by default, you are redirected to the auto-generated &lt;a href=&#34;https://swagger.io/docs/specification/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAPI&lt;/a&gt; API documentation (yes, you get this &lt;em&gt;for free&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;To see the results for a specific accession, click here: http://localhost:8000/geo/GSM48743. An example response will look like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;GSM48743&amp;quot;: {
        &amp;quot;title&amp;quot;: &amp;quot;Patient sample ST163, Liposarcoma&amp;quot;,
        &amp;quot;status&amp;quot;: &amp;quot;Public on Oct 11 2005&amp;quot;,
        &amp;quot;submission_date&amp;quot;: &amp;quot;2005-04-21&amp;quot;,
        &amp;quot;last_update_date&amp;quot;: &amp;quot;2005-11-22&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;RNA&amp;quot;,
        &amp;quot;anchor&amp;quot;: null,
        &amp;quot;contact&amp;quot;: {
            &amp;quot;city&amp;quot;: &amp;quot;Bethesda&amp;quot;,
            &amp;quot;name&amp;quot;: {
                &amp;quot;first&amp;quot;: &amp;quot;Sean&amp;quot;,
                &amp;quot;middle&amp;quot;: &amp;quot;&amp;quot;,
                &amp;quot;last&amp;quot;: &amp;quot;Davis&amp;quot;
            },
            &amp;quot;email&amp;quot;: &amp;quot;sdavis2@mail.nih.gov&amp;quot;,
            &amp;quot;state&amp;quot;: &amp;quot;MD&amp;quot;,
            &amp;quot;address&amp;quot;: &amp;quot;37 Convent Drive, Room 6138&amp;quot;,
            &amp;quot;department&amp;quot;: null,
            &amp;quot;country&amp;quot;: &amp;quot;USA&amp;quot;,
            &amp;quot;web_link&amp;quot;: null,
            &amp;quot;institute&amp;quot;: &amp;quot;National Cancer Institute&amp;quot;,
            &amp;quot;zip_postal_code&amp;quot;: null,
            &amp;quot;phone&amp;quot;: &amp;quot;301-435-2652&amp;quot;
        },
        &amp;quot;description&amp;quot;: &amp;quot;Diagnosis: Liposarcoma\nKeywords = sarcoma, cDNA, Liposarcoma&amp;quot;,
        &amp;quot;accession&amp;quot;: &amp;quot;GSM48743&amp;quot;,
        &amp;quot;biosample&amp;quot;: null,
        &amp;quot;tag_count&amp;quot;: null,
        &amp;quot;tag_length&amp;quot;: null,
        &amp;quot;platform_id&amp;quot;: &amp;quot;GPL1977&amp;quot;,
        &amp;quot;hyb_protocol&amp;quot;: null,
        &amp;quot;channel_count&amp;quot;: 2,
        &amp;quot;scan_protocol&amp;quot;: null,
        &amp;quot;data_row_count&amp;quot;: 12600,
        &amp;quot;library_source&amp;quot;: null,
        &amp;quot;overall_design&amp;quot;: null,
        &amp;quot;sra_experiment&amp;quot;: null,
        &amp;quot;data_processing&amp;quot;: null,
        &amp;quot;supplemental_files&amp;quot;: [
            &amp;quot;NONE&amp;quot;
        ],
        &amp;quot;channels&amp;quot;: [
            {
                &amp;quot;label&amp;quot;: null,
                &amp;quot;taxid&amp;quot;: [
                    9606
                ],
                &amp;quot;molecule&amp;quot;: &amp;quot;total RNA&amp;quot;,
                &amp;quot;organism&amp;quot;: &amp;quot;Homo sapiens&amp;quot;,
                &amp;quot;source_name&amp;quot;: &amp;quot;Liposarcoma&amp;quot;,
                &amp;quot;label_protocol&amp;quot;: null,
                &amp;quot;growth_protocol&amp;quot;: null,
                &amp;quot;extract_protocol&amp;quot;: null,
                &amp;quot;treatment_protocol&amp;quot;: null,
                &amp;quot;characteristics&amp;quot;: [
                    
                ]
            },
            {
                &amp;quot;label&amp;quot;: null,
                &amp;quot;taxid&amp;quot;: [
                    9606
                ],
                &amp;quot;molecule&amp;quot;: &amp;quot;total RNA&amp;quot;,
                &amp;quot;organism&amp;quot;: &amp;quot;Homo sapiens&amp;quot;,
                &amp;quot;source_name&amp;quot;: &amp;quot;Pooled sarcoma cell lines&amp;quot;,
                &amp;quot;label_protocol&amp;quot;: null,
                &amp;quot;growth_protocol&amp;quot;: null,
                &amp;quot;extract_protocol&amp;quot;: null,
                &amp;quot;treatment_protocol&amp;quot;: null,
                &amp;quot;characteristics&amp;quot;: [
                    
                ]
            }
        ],
        &amp;quot;contributor&amp;quot;: [
            
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the web application is running locally. We can
&amp;ldquo;containerize&amp;rdquo; the application by building the docker container. The
&lt;a href=&#34;https://github.com/seandavi/blog-code/blob/master/geo-metadata-cloud-run/Dockerfile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;associated Dockerfile&lt;/a&gt; has instructions for doing so. To actually
build the docker image, do the following, replacing &lt;code&gt;PROJECT_ID&lt;/code&gt; below with
your GCP project ID. You can view your project ID by running the
command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud config get-value project.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PROJECT_ID=&#39;my-google-project-name&#39;
docker build -t gcr.io/${PROJECT_ID}/geo-cloud-run .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Push the docker image to the Google Cloud Project image
repository. You can also use dockerhub name/url if you like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker push gcr.io/${PROJECT_ID}/geo-cloud-run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the app as a dockerized app locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 8000:80 gcr.io/${PROJECT_ID}/geo-cloud-run 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, you should now be able to access the API at http://localhost:8000/docs.&lt;/p&gt;
&lt;p&gt;One more step, deployment, gets us the application running in the
cloud as a serverless application. Deploy using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud run deploy --image gcr.io/${PROJECT_ID}/geo-cloud-run --platform managed
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;You will be prompted for the service name: press Enter to accept the
default name, geo-cloud-run&lt;/li&gt;
&lt;li&gt;You will be prompted for region: select the region of your choice,
for example us-central1.&lt;/li&gt;
&lt;li&gt;You will be prompted to allow unauthenticated invocations: respond &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then wait a few moments until the deployment is complete. On success,
the command line displays the service URL. If you navigate to that
service URL, you will be accessing your &lt;em&gt;entirely serverless&lt;/em&gt;
application. Build more complicated applications by simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adjust python code.&lt;/li&gt;
&lt;li&gt;Rebuild docker image.&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;gcloud run update ....&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When completed, simply run &lt;code&gt;gcloud run delete ....&lt;/code&gt; to remove your
service and application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building R Binary Packages for Linux</title>
      <link>/post/build-linux-r-binary-packages/</link>
      <pubDate>Mon, 14 Oct 2019 12:22:47 -0400</pubDate>
      <guid>/post/build-linux-r-binary-packages/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;One of the challenges of producing a performant build environment for linux, such as what might
be used to have developers test software in &lt;em&gt;identical&lt;/em&gt; environments, is the need to compile
R packages from source on linux. If, however, one had an identical set of installed libraries, kernel
version, compiler, etc., we could use binary packages in linux as well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docker.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt; provides just such a shareable and identical environment for linux. Recent work by
Levi Waldron and Nitesh Turaga to produce the &lt;a href=&#34;https://hub.docker.com/r/bioconductor/bioconductor_full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bioconductor_full&lt;/a&gt; docker image will allow for
nearly &lt;strong&gt;all&lt;/strong&gt; bioconductor packages to be installed, as the underlying system dependencies
are all included.&lt;/p&gt;
&lt;h2 id=&#34;docs-from-r-on-building-binaries&#34;&gt;Docs from R on building binaries&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.hep.by/gnu/r-patched/r-exts/R-exts_20.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docs on building binary R packages&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The recommended method of building binary packages is to use
&lt;code&gt;R CMD INSTALL --build pkg&lt;/code&gt; where &lt;code&gt;pkg&lt;/code&gt; is either the name of a source tarball (in the usual &lt;code&gt;.tar.gz&lt;/code&gt; format) or the location of the directory of the package source to be built.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;R CMD INSTALL --build&lt;/code&gt; operates by first installing the package and then packing the installed binaries into the appropriate binary package file for the particular platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;By default, &lt;code&gt;R CMD INSTALL --build&lt;/code&gt; will attempt to install the package into the default library tree for the local installation of R. This has two implications:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;If the installation is successful, it will overwrite any existing installation of the same package. The default library tree must have write permission; if not, the package will not install and the binary will not be created. To prevent changes to the present working installation or to provide an install location with write access, create a suitably located directory with write access and use the -l option to build the package in the chosen location. The usage is then&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;R CMD INSTALL -l location --build pkg
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;where &lt;code&gt;location&lt;/code&gt; is the chosen directory with write access. The package will be installed as a subdirectory of &lt;code&gt;location&lt;/code&gt;, and the package binary will be created in the current directory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;p&gt;With that background in place, by starting a docker container from bioconductor_full, we can
build binary packages that can be shared with others who are also running using bioconductor_full.&lt;/p&gt;
&lt;p&gt;The next command assumes that docker is running.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -v PATH_TO_LOCAL_STORAGE_DIRECTORY:/data \
  --name bioc_full \
  -e PASSWORD=&amp;lt;YOUR_PASSWORD_OF_CHOICE&amp;gt; \
  -p 8787:8787 \
  bioconductor/bioconductor_full:devel 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PATH_TO_LOCAL_STORAGE_DIRECTORY&lt;/code&gt; should be replaced with the local directlry
where the binary packages will land as they are built inside the container. Packages can
then be reused or copied somewhere else for installation as binaries.&lt;/p&gt;
&lt;p&gt;After running the &lt;code&gt;docker run&lt;/code&gt; command above, you should be able to navigate to
https://localhost:8787/ (or whatever your docker host address is). You will be presented
with an Rstudio login. Login with username=&lt;code&gt;rstudio&lt;/code&gt; and
password=&lt;code&gt;YOUR_PASSWORD_OF_CHOICE&lt;/code&gt; as set above.&lt;/p&gt;
&lt;h2 id=&#34;install-and-build-binaries&#34;&gt;Install and build binaries&lt;/h2&gt;
&lt;p&gt;Binary packages, after being installed and built, will be placed in the current working
directory. I switch to the directory that is mapped back to the host so that I can
keep the binary packages around after the container stops.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;setwd(&#39;/data&#39;) # to drop binary tarballs into this directory
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After logging into Rstudio, execute the following command. Note the &lt;code&gt;INSTALL_opts&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;# Biocmanager will be installed already for bioconductor_full
BiocManager::install(&#39;limma&#39;, INSTALL_opts=&#39;--build&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These installation options will copy the installed binary package(s) to &lt;code&gt;/data&lt;/code&gt;. These will end
up on the docker host machine in the &lt;code&gt;PATH_TO_LOCAL_STORAGE_DIRECTORY&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;Bioconductor version 3.10 (BiocManager 1.30.4), R 3.6.1 (2019-07-05)
Installing package(s) &#39;limma&#39;
trying URL &#39;https://bioconductor.org/packages/3.10/bioc/src/contrib/limma_3.41.18.tar.gz&#39;
Content type &#39;application/x-gzip&#39; length 1493044 bytes (1.4 MB)
==================================================
downloaded 1.4 MB

* installing *source* package ‘limma’ ...
** using staged installation
** libs
gcc -I&amp;quot;/usr/local/lib/R/include&amp;quot; -DNDEBUG   -I/usr/local/include  -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init.c -o init.o
gcc -I&amp;quot;/usr/local/lib/R/include&amp;quot; -DNDEBUG   -I/usr/local/include  -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c normexp.c -o normexp.o
gcc -I&amp;quot;/usr/local/lib/R/include&amp;quot; -DNDEBUG   -I/usr/local/include  -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c weighted_lowess.c -o weighted_lowess.o
gcc -shared -L/usr/local/lib/R/lib -L/usr/local/lib -o limma.so init.o normexp.o weighted_lowess.o -L/usr/local/lib/R/lib -lR
installing to /usr/local/lib/R/site-library/00LOCK-limma/00new/limma/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* creating tarball
packaged installation of ‘limma’ as ‘limma_3.41.18_R_x86_64-pc-linux-gnu.tar.gz’
* DONE (limma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check what we created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;dir(&#39;/data&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your version of limma may differ.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34;&gt;[1] &amp;quot;limma_3.41.18_R_x86_64-pc-linux-gnu.tar.gz&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These binary packages can be installed just like any &lt;code&gt;.tar.gz&lt;/code&gt; package but will
be intalled very quickly like on Mac OS and Windows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remember to kill the docker container after you are done&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Container Notes</title>
      <link>/docs/container-notes/</link>
      <pubDate>Fri, 04 Oct 2019 17:04:04 -0400</pubDate>
      <guid>/docs/container-notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using google cloud registry for private docker images</title>
      <link>/post/using-google-cloud-registry/</link>
      <pubDate>Wed, 06 Feb 2019 13:12:54 -0500</pubDate>
      <guid>/post/using-google-cloud-registry/</guid>
      <description>


&lt;p&gt;In this post, I will quickly build a docker image containing the
sra-toolkit and a key for dbGaP downloads. Because the key file is
private, I will be using the secure Google Container Registry to store
the image for later use in genomics workflows.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Container technologies like &lt;a href=&#34;https://docker.io/&#34;&gt;docker&lt;/a&gt; enable quick and easy
encapsulation of software, dependencies, and operating systems. One or
more containers can render entire software ecosystems portable,
enhance reproducibility and reusability, and facilitate sharing of
software, tools, and even infrastructure.&lt;/p&gt;
&lt;p&gt;While &lt;a href=&#34;https://hub.docker.com/&#34;&gt;DockerHub&lt;/a&gt; is perhaps the most well-known registry where such
docker images can be housed, others such as &lt;a href=&#34;https://quay.io/&#34;&gt;quay.io&lt;/a&gt; are also
available. Commercial cloud environments, such as the &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud
Platform&lt;/a&gt; often offer their own registries that use the same secure
access controls as other cloud services, &lt;em&gt;allowing docker images with
proprietary or private information to be stored and accessed
securely&lt;/em&gt;. They are also typically quite integrated with commercial
cloud services (&lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;gcr docs&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I am experimenting with the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Google Container Registry&lt;/a&gt; (GCR) for a
bioinformatics project that I plan to perform in on Google Cloud. This
blog post simply serves as notes to myself about details of using that
system.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;A Google Cloud &lt;a href=&#34;https://cloud.google.com/container-registry/docs/quickstart&#34;&gt;account and project&lt;/a&gt; is required to follow along here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/sdk/docs/&#34;&gt;Install the Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/&#34;&gt;Install
Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to allow docker to use google for authorization, we need to
do this one-time command to get tie docker to google.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;gcloud auth configure-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer “yes” to the prompt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-docker-image-and-adding-to-gcr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building docker image and adding to GCR&lt;/h2&gt;
&lt;p&gt;We are going to build a simple docker file that includes the
&lt;a href=&#34;https://github.com/ncbi/sra-tools&#34;&gt;sra-tools&lt;/a&gt; package, import a private key for downloading files that
are protected, and then store that image to GCR.&lt;/p&gt;
&lt;p&gt;The Dockerfile is given below. Note that I &lt;strong&gt;am not including the dbGaP
key file, as it is private&lt;/strong&gt;, but you could modify the Dockerfile to
include your own key file(s) or simply remove the dbGaP access details
entirely.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/seandavi/410cfbbed6b8db0b4928eb36236e7dda.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Building the container is one line:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker build -t sratoolkit .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the docker image has been created. Run it locally to
test, for example.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -ti sratoolkit /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the docker container, all the sra-toolkit binaries are
available.&lt;/p&gt;
&lt;p&gt;Each Google Cloud Project has a private GCR. Therefore, GCR urls
include the &lt;code&gt;PROJECT-ID&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker tag sratoolkit gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2
docker push gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;For fun, we can use the image and the &lt;code&gt;fastq-dump&lt;/code&gt; utility to download
a single SRA run. The docker image will only run as long as necessary
to perform the fastq dump and then will terminate. This post is not
about the details of running docker, but note that the following will
result in the fastq file from the &lt;code&gt;fastq-dump&lt;/code&gt; command ending up in
the &lt;code&gt;/tmp&lt;/code&gt; directory on your machine.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -v /tmp:/data -ti  gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2\
    /bin/bash -c &amp;quot;cd /root/ncbi/dbGaP-16049/ \
    &amp;amp;&amp;amp; fastq-dump   --split-files --gzip \
    --skip-technical -X 10000   SRR390728&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will result in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Read 10000 spots for SRR390728
Written 10000 spots for SRR390728&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;best-practices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Best practices&lt;/h2&gt;
&lt;p&gt;Storing keys, key files, and any other private information in a docker
image is a risky operation and is not a best practice. Leveraging the
security model of GCR mitigates these issues. However, it is &lt;strong&gt;really
easy&lt;/strong&gt; to forget about the information that might be leaked if this
image were shared or pushed to a public repository like DockerHub.&lt;/p&gt;
&lt;p&gt;For a discussion of other models for accessing private information
inside a docker container, see &lt;a href=&#34;https://www.ctl.io/developers/blog/post/tutorial-protecting-sensitive-info-docker&#34;&gt;this blog post, for example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From here&lt;/h2&gt;
&lt;p&gt;At this point, we have created an environment for building docker
images, stored a proprietary docker image in the Google Cloud
Registry, and successfully used the image to run a simple command to
perform a &lt;code&gt;fastq-dump&lt;/code&gt; with the files ending up on the docker host
machine. This image can be used anywhere a docker image can run, but
only if google authentication &lt;a href=&#34;#preliminaries&#34;&gt;has been tied into docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/genomics/docs/quickstart&#34;&gt;Google Genomics Pipeline API&lt;/a&gt; is built around scalable genomic
workflows and uses docker images for workflow tasks. The image here
can be used for dbGaP genomic data extraction as part of such a
workflow, all within the secure environment of google cloud.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/google-kubernetes-autoscale-with-preemptible-instances/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/google-kubernetes-autoscale-with-preemptible-instances/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export POOL_NAME=&#39;preempt-1&#39;
export CLUSTER_NAME=&#39;cluster-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud beta container node-pools create ${POOL_NAME} --preemptible \
	   --cluster ${CLUSTER_NAME} --enable-autoscaling \
	   --min-nodes=0 --max-nodes=50 \
	   --machine-type=n1-standard-2                                    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Job
spec:
  nodeSelector:
    cloud.google.com/gke-preemptible: &amp;quot;true&amp;quot;
  template:
    spec:
      containers:
      - name: pyversion
        image: python:3.7
        command: [&amp;quot;python&amp;quot;,  &amp;quot;--version&amp;quot;]
      restartPolicy: Never
  backoffLimit: 4
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
