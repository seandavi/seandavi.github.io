<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data science | seandavi(s12)</title>
    <link>/tag/data-science/</link>
      <atom:link href="/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>data science</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 31 Jan 2020 18:00:00 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>data science</title>
      <link>/tag/data-science/</link>
    </image>
    
    <item>
      <title>Experimenting with Github Actions</title>
      <link>/post/message-queues-to-orchestrate-hpc-batch-runs/</link>
      <pubDate>Fri, 31 Jan 2020 18:00:00 -0400</pubDate>
      <guid>/post/message-queues-to-orchestrate-hpc-batch-runs/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Actions&lt;/a&gt; allow flexible and potentially complicated actions
that comprise workflows that respond to &lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt; on Github. Continuous
integration, messaging Slack, greeting new contributors, deploying
applications, and many other templates are ready for customization and
integration into any repo.&lt;/p&gt;
&lt;p&gt;As of this writing, GitHub Actions are available as a &lt;em&gt;beta&lt;/em&gt; feature,
requiring &lt;a href=&#34;https://github.com/features/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;signup&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definitionshttpshelpgithubcomenarticlesabout-github-actionscore-concepts-for-github-actions&#34;&gt;&lt;a href=&#34;https://help.github.com/en/articles/about-github-actions#core-concepts-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Definitions&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;action&#34;&gt;Action&lt;/h3&gt;
&lt;p&gt;An action is the smallest portable building block of a
workflow. Actions are combined as steps to form a workflow. Actions can be
created from scratch, modified from &lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly available actions&lt;/a&gt;, and
shared with others.&lt;/p&gt;
&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;
&lt;p&gt;A workflow is a collection of steps (actions) that describe a process
that will execute in response to GitHub &lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt;. A YAML
file that defines your workflow configuration with at least one
job. This file lives in the root of your GitHub repository in the
&lt;code&gt;.github/workflows&lt;/code&gt; directory. Multiple workflow files may exist and
all will be active.&lt;/p&gt;
&lt;h3 id=&#34;step&#34;&gt;Step&lt;/h3&gt;
&lt;p&gt;A step is a set of tasks performed by a job. Each step in a job
executes in the same virtual environment, allowing the actions in that
job to share information using the filesystem. Steps can run commands
(shell commands, etc.) or actions.&lt;/p&gt;
&lt;h3 id=&#34;virtual-environment&#34;&gt;Virtual environment&lt;/h3&gt;
&lt;p&gt;GitHub has hosts for Linux, macOS, and Windows that can serve as
execution environments for workflows.&lt;/p&gt;
&lt;h3 id=&#34;runner&#34;&gt;Runner&lt;/h3&gt;
&lt;p&gt;Jobs run on a service that waits in virtual environment that waits for available
jobs. The runner takes jobs one-at-a-time and runs each, reporting
logs, etc., back to Github.&lt;/p&gt;
&lt;h3 id=&#34;event&#34;&gt;Event&lt;/h3&gt;
&lt;p&gt;Workflows can be triggered by a [repository event] such as a push, a
new issue, etc.&lt;/p&gt;
&lt;h3 id=&#34;artifact&#34;&gt;Artifact&lt;/h3&gt;
&lt;p&gt;Artifacts are the files that are created during a workflow. These can
be deployed, used in other workflows, published, etc. Additional
actions allow working with artifacts.&lt;/p&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;h3 id=&#34;create-repo&#34;&gt;Create repo&lt;/h3&gt;
&lt;p&gt;Here, I simulate creating a simple python package by forking &lt;a href=&#34;https://github.com/bast/somepackage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
example repo&lt;/a&gt; by
&lt;a href=&#34;https://github.com/bast&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bast&lt;/a&gt;. The package is to exemplify some best
practices for structuring python code. However, here it serves as a
simple example of a formal python package that we can build and test.&lt;/p&gt;
&lt;p&gt;Login to Github, navigate to the &lt;a href=&#34;https://github.com/bast/somepackage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example repo&lt;/a&gt;, and &lt;a href=&#34;https://help.github.com/en/articles/fork-a-repo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fork the
repo&lt;/a&gt;.&lt;/p&gt;


















&lt;figure id=&#34;figure-fork-the-example-repohttpsgithubcombastsomepackage&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;fork-repo.png&#34; data-caption=&#34;Fork the &amp;lt;a href=&amp;#34;https://github.com/bast/somepackage&amp;#34;&amp;gt;example repo&amp;lt;/a&amp;gt;.&#34;&gt;


  &lt;img src=&#34;fork-repo.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fork the &lt;a href=&#34;https://github.com/bast/somepackage&#34;&gt;example repo&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After forking, a fresh package will be available in your repository.&lt;/p&gt;
&lt;h3 id=&#34;add-a-github-action&#34;&gt;Add a github action&lt;/h3&gt;
&lt;p&gt;After ensuring that you have [signed up] for Github Actions and have
verified that you are &amp;ldquo;in&amp;rdquo; by noticing the &lt;code&gt;Actions&lt;/code&gt; button at the top
of your github repository:&lt;/p&gt;


















&lt;figure id=&#34;figure-the-github-actions-button-should-be-present-in-order-to-proceed&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;action-button.png&#34; data-caption=&#34;The Github Actions button should be present in order to proceed.&#34;&gt;


  &lt;img src=&#34;action-button.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Github Actions button should be present in order to proceed.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now, we can proceed with adding a workflow from the set of recommended
workflow templates (or others). Clicking on the &lt;code&gt;Actions&lt;/code&gt; button will
bring up the following screen.&lt;/p&gt;


















&lt;figure id=&#34;figure-choose-the-python-package-workflow&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;choose-action.png&#34; data-caption=&#34;Choose the python package workflow.&#34;&gt;


  &lt;img src=&#34;choose-action.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Choose the python package workflow.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Click on the &lt;code&gt;Set up this workflow&lt;/code&gt; on the &amp;ldquo;Python package&amp;rdquo;
card. That will bring up the following screen.&lt;/p&gt;


















&lt;figure id=&#34;figure-the-workflow-editor&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;workflow-editor.png&#34; data-caption=&#34;The workflow editor.&#34;&gt;


  &lt;img src=&#34;workflow-editor.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The workflow editor.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;What is happening at this point is that Github is about to add a new
file to your repository. The file will be located at
&lt;code&gt;somepackage/.github/workflows&lt;/code&gt;. Edit the filename as you see
fit.&lt;/p&gt;
&lt;p&gt;The contents of the workflow file will look like the listing below by
default and follows &lt;a href=&#34;https://help.github.com/en/articles/workflow-syntax-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the workflow
syntax&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;name: Python package

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [2.7, 3.5, 3.6, 3.7]

    steps:
    - uses: actions/checkout@v1
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Lint with flake8
      run: |
        pip install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest
      run: |
        pip install pytest
        pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;commit-new-workflow-file&#34;&gt;Commit new workflow file&lt;/h3&gt;
&lt;p&gt;Once you have made any edits (no need to, though), go ahead and click
the &lt;code&gt;Start Commit&lt;/code&gt; button.&lt;/p&gt;


















&lt;figure id=&#34;figure-make-the-first-commit&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;first-commit.png&#34; data-caption=&#34;Make the first commit.&#34;&gt;


  &lt;img src=&#34;first-commit.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Make the first commit.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;build-results&#34;&gt;Build results&lt;/h3&gt;
&lt;p&gt;By navigating back to &lt;code&gt;Actions&lt;/code&gt;, you should see that the workflow is
running or has already completed. An example of what a running
workflow might look like is below.&lt;/p&gt;


















&lt;figure id=&#34;figure-image-credit-github-bloghttpsgithubblog2019-08-08-github-actions-now-supports-ci-cd&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.blog/wp-content/uploads/2019/08/streaming.gif?w=918&amp;amp;resize=918%2C802&#34; data-caption=&#34;Image Credit: &amp;lt;a href=&amp;#34;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&amp;#34;&amp;gt;GitHub Blog&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;https://github.blog/wp-content/uploads/2019/08/streaming.gif?w=918&amp;amp;resize=918%2C802&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Image Credit: &lt;a href=&#34;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&#34;&gt;GitHub Blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You are free to investigate the logs, etc.&lt;/p&gt;
&lt;h3 id=&#34;modifying-the-workflow&#34;&gt;Modifying the workflow&lt;/h3&gt;
&lt;p&gt;Modifying the workflow is as simple as making edits to the
&lt;code&gt;.github/workflows/...yaml&lt;/code&gt; file. For example, you might consider
changing the name of the workflow and modifying the python versions
that are used for testing.&lt;/p&gt;
&lt;h3 id=&#34;workflow-status-badge&#34;&gt;Workflow status badge&lt;/h3&gt;
&lt;p&gt;Follow &lt;a href=&#34;https://help.github.com/en/articles/configuring-a-workflow#adding-a-workflow-status-badge-to-your-repository&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these
instructions&lt;/a&gt;
to add a status badge to your repo that will be updated with each
workflow execution. Example code is here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[![Actions
Status](https://github.com/seandavi/somepackage/workflows/Python%20package/badge.svg)](https://github.com/seandavi/somepackage/actions)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;%20&lt;/code&gt; that represents a url-encoded &lt;code&gt;space&lt;/code&gt; character. The resulting badge looks like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/seandavi/somepackage/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://github.com/seandavi/somepackage/workflows/Python%20package/badge.svg&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Github actions and workflows add continuous integration and other
capabilities that are configurable via text files that are captured
and versioned alongside code. Therefore, actions can be shared,
modified, and manipulated with standard text editors, etc.&lt;/p&gt;
&lt;p&gt;Many actions are &lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly
available&lt;/a&gt;. Example workflows are also
&lt;a href=&#34;https://github.com/actions/starter-workflows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easy to find&lt;/a&gt;. For
those who are partial to &lt;em&gt;R&lt;/em&gt;, there is the R-centric
&lt;a href=&#34;https://r-lib.github.io/ghactions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ghactions&lt;/a&gt; package that 1)
provides workflow templates for common R projects (packages,
RMarkdown, …) with sensible defaults and 2) wraps and curates relevant
existing external actions, such as those to deploy to GitHub Pages or
Netlify.&lt;/p&gt;
&lt;p&gt;Hardware available as of this writing is &lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#supported-virtual-environments-and-hardware-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;described
here&lt;/a&gt;
and includes Windows, Linux, and MacOS environments. When a workflow
is running, multiple &lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#environment-variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;environment
variables&lt;/a&gt;
are set and accessible to software. Github actions allow the user to
create
&lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#creating-and-using-secrets-encrypted-variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;secrets&lt;/a&gt;
to securely store credentials for use inside run environments. Github
actions are container-ready, allowing &lt;a href=&#34;https://help.github.com/en/articles/creating-a-docker-container-action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;containerized
actions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post, like many of my posts, is very operational, but I have
found that a worked example is usually more valuable than long
expository posts. That said, I always appreciate comments and
suggestions for improvements (see below).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Package Registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/articles/workflow-syntax-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workflow Syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly available actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.community/t5/GitHub-Actions/bd-p/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Community support site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Container Notes</title>
      <link>/docs/container-notes/</link>
      <pubDate>Fri, 04 Oct 2019 17:04:04 -0400</pubDate>
      <guid>/docs/container-notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bioconductor: software for interpreting high-throughput biological data</title>
      <link>/talk/2018-12-03-wake-forest-bioconductor/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 -0500</pubDate>
      <guid>/talk/2018-12-03-wake-forest-bioconductor/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQI77sKlJSlFIjaF-c7CjXDaaCoB5wA_ly63ObRkGS2I2NDSTTLV-crI5XgjfefnFQ_QM4wRe2eAqh2/embed?start=true&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Practical Data Science and Informatics Training</title>
      <link>/talk/2018-12-3-wake-forest-biomedical-informatics-datasci-ed/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 -0500</pubDate>
      <guid>/talk/2018-12-3-wake-forest-biomedical-informatics-datasci-ed/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vRGJiv0PbsSpytcoiPClSlZs7JhztGPiKOd-GptnFDaOemA6KPpYac71ATSDsDtdhOLk1Cm10-f4bBA/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The cancer data ecosystem: data and cloud resources for cancer genomic data science</title>
      <link>/talk/2018-10-16-cancer-data-ecosystem-cmu/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 -0400</pubDate>
      <guid>/talk/2018-10-16-cancer-data-ecosystem-cmu/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSksBmgOBEjIdxxATW3fWG-EZoULxDc-tg94y8QdNdKa6NPnDWI6el8yHRSrXDLr09wDRNy4n-xeEqR/embed?start=true&amp;amp;loop=true&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Create a basic Apache Spark cluster in the cloud (in 5 minutes)</title>
      <link>/post/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/post/create-a-basic-apache-spark-cluster-in-the-cloud-in-5-minutes/</guid>
      <description>


&lt;div id=&#34;apache-spark-in-a-few-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apache Spark in a few words&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; is a software and data science platform that is
purpose-built for large- to massive-scale data processing. Spark
supports processing of data in batch mode (run as a pipeline) or in
interactive mode using command-line programming style or in popular
notebook style of coding. While &lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;scala&lt;/a&gt; is the native language for
Spark, language bindings exist for python, R, and Java as well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spark.apache.org&#34; &gt;
&lt;img
src=&#34;https://www.onlinebooksreview.com/uploads/blog_images/2017/11/27_file.png&#34;
 style=&#34;width:300px;float:left;padding-right:10px&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spark is built around an underlying data abstraction called Resilient
Distributed Dataset, or RDD. The RDD represents an immutable,
distributed, and partitioned collection of elements that can be
operated on in parallel. The collection of elements in the RDD need
not fit into memory, though the performance is maximal when the RDD
fits into the Spark “cluster” memory.&lt;/p&gt;
&lt;p&gt;Spark is capable of processing
data &lt;a href=&#34;https://spark.apache.org/faq.html&#34;&gt;at very large scales&lt;/a&gt;. That
said, code for Spark need not be written on a large cluster. Spark can
be deployed on a laptop as well, facilitating code development and
testing at a small scale.&lt;/p&gt;
&lt;p&gt;I am not going to delve into working with Spark just yet. Rather, I am
starting with a quick walkthrough of creating a Spark cluster on the
Amazon Web Services Elastic Map Reduce
service &lt;a href=&#34;https://aws.amazon.com/emr/&#34;&gt;AWS EMR&lt;/a&gt;. This is not a
recommendation of AWS over other potential providers and choices in
the following workflow are &lt;em&gt;not&lt;/em&gt; meant as best practices. This is just
a documentation of the process with a little text.&lt;/p&gt;
&lt;style&gt;
img {
  width: 100%
    }
.alert {
  background-color: #fcc2c2;
  border-radius: 10px;
  padding: 5px;
  padding-left: 20px;
  padding-right: 20px;
  }
&lt;/style&gt;
&lt;div class=&#34;alert&#34;&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This blog post creates resources on a commercial cloud which
will continue to cost money until they are terminated. In
order to keep from getting charged for unused resources, be sure to
clean up by terminating the resources once you are done.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The main prerequisite is an AWS account.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Walk through creating an Apache Spark cluster on AWS using the EMR
service.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-go&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s go&lt;/h2&gt;
&lt;p&gt;Login to your AWS &lt;a href=&#34;https://console.aws.amazon.com&#34;&gt;console&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After logging in, you should see a window with a lot of AWS services
listed. At the top left, choose the “Services” button and type “EMR”
into the search box. Then, choose EMR.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/chooseEMRService.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the next screen, choose “Create Cluster” by clicking the blue
button. Just a note that the “AWS Glue Catalog” that is featured
prominently in a couple of places in the configuration is a separatemarkdow
service from AWS, &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html&#34;&gt;detailed here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/createcluster.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On this screen, choose an arbitrary name for your cluster. You can
choose no logging for now, or specify a logging s3 bucket/path if you
like. Change the software configuration to Spark as shown (version
numbers may differ–that is OK).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/configuration2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next step is to configure the hardware that will comprise the
cluster. Choosing the appropriate size and number of machines while
balancing costs is an art form that I have not mastered. However, you
will likely want at least a master and one worker (specify 2 or
more). Starting with the defaults for “experimentation” is probably
not bad. Roughly speaking, you’ll want enough memory on your cluster
to support keeping your datasets in memory for maximum performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/hardwareconfigbasic.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike many commercially-offered online services, AWS EMR is not, by
default, configured for “open” access. To gain access to the cluster,
you will need to provide an SSH key &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair&#34;&gt;see her for how to generate an
ssh key on AWS&lt;/a&gt; that
you will later use to enable access to the Spark notebook and
web-based user interface for monitoring. Assuming that you have an SSH
key created, choose that key in the dropdown as pictured below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/sshkey.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Click the “Create Cluster” at the bottom right of the screen. You
should then be presented with a page that shows the cluster
details.&lt;/p&gt;
&lt;p&gt;On AWS, cluster creation takes several minutes to up to 30 minutes. My
only other experience with cloud Spark-as-a-service is on Google Cloud
Platform which has a much faster startup time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/startingup.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After a few minutes, you will have a running Apache Spark cluster that
you can begin to experiment with. That will need to wait for another
post, but to gain access to the cluster, including a Zeppelin notebook
(quite similar to Jupyter), click the “Enable Web Connection” and
follow the instructions (a little involved, including establishing a
proxy connection and installing a proxy plugin for your browser).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spark-intro/enablesshconn.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cleanup-very-important&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleanup: VERY IMPORTANT&lt;/h2&gt;
&lt;div class=&#34;alert&#34;&gt;
&lt;p&gt;After you have created the Spark cluster, it &lt;strong&gt;costs money&lt;/strong&gt; until you
destroy it. &lt;em&gt;Please do not forget&lt;/em&gt; to click “Terminate” and then check
back to &lt;em&gt;make doubly sure&lt;/em&gt; that the cluster is terminated.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matched tumor/normal pairs--a use case for the GenomicDataCommons Bioconductor package</title>
      <link>/post/2017-03-04-testing-the-genomicdatacommons-package/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/post/2017-03-04-testing-the-genomicdatacommons-package/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The NCI Genomic Data Commons (&lt;a href=&#34;https://gdc.nci.nih.gov&#34;&gt;GDC&lt;/a&gt;) is a reboot of the approach that NCI uses to manage and expose genomic and associated clinical and experimental metadata. I have been working on a &lt;a href=&#34;https://bioconductor.org&#34;&gt;Bioconductor&lt;/a&gt; package that interfaces with the &lt;a href=&#34;https://gdc-api.nci.nih.gov&#34;&gt;GDC API&lt;/a&gt; to provide search and data retrieval from within R.&lt;/p&gt;
&lt;div id=&#34;testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;testing&lt;/h2&gt;
&lt;p&gt;In the first of what will likely be a set of use cases for the &lt;a href=&#34;https://github.com/Bioconductor/GenomicDataCommons&#34;&gt;GenomicDataCommons&lt;/a&gt;, I am going to address a question that came up on twitter from &lt;a href=&#34;https://twitter.com/sleight82&#34;&gt;@sleight82&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/sleight82/status/837898737540198400&#34;&gt;Asking for a non-tweeter: can you find matched control samples?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;asking for a non-tweeter: can you find matched control samples?&lt;/p&gt;&amp;mdash; Kenneth Daily (@sleight82) &lt;a href=&#34;https://twitter.com/sleight82/status/837898737540198400?ref_src=twsrc%5Etfw&#34;&gt;March 4, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The answer is, “Yes.” I am going to assume that what the “non-tweeter” friend meant was that he or she was interested in finding matched tumor/normal data related to, for example, gene expression, and that the interest is in a specific disease category or TCGA. So, I am going to answer the more specific question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can you find matched primary tumor/solid tissue normal samples and associated RNA-Seq gene expression files from TCGA breast cancer cases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GenomicDataCommons)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: magrittr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;GenomicDataCommons&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:GenomicDataCommons&amp;#39;:
## 
##     count, filter, select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The GDC API puts some constraints on what can be done directly. But, since we are working in R, we have a toolbox that allows us to build a solution out of component parts. The strategy that I am going to employ is a three-step approach&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find RNA-Seq gene expression files derived from “Solid Tissue Normal”&lt;/li&gt;
&lt;li&gt;Find RNA-Seq gene expression files derived from “Primary Tumor”&lt;/li&gt;
&lt;li&gt;Limit cases from #1 and #2 that have gene expression results from both normal and tumor.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this code block, find all “HTSeq - Counts” files that are derived from “Solid Tissue Normal” in the project “TCGA-BRCA”. I used a combination of &lt;code&gt;files() %&amp;gt;% select(available_fields(&#39;files&#39;) %&amp;gt;% results()&lt;/code&gt; to get an overview of the data available in the &lt;code&gt;files()&lt;/code&gt; endpoint, followed by &lt;code&gt;grep_fields()&lt;/code&gt; and &lt;code&gt;available_values()&lt;/code&gt; to determine how to build filters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nl_ge_files = files() %&amp;gt;%
    GenomicDataCommons::filter(~   cases.samples.sample_type==&amp;#39;Solid Tissue Normal&amp;#39; &amp;amp;
               cases.project.project_id == &amp;#39;TCGA-BRCA&amp;#39; &amp;amp;
               analysis.workflow_type == &amp;quot;HTSeq - Counts&amp;quot;) %&amp;gt;%
    expand(c(&amp;#39;cases&amp;#39;,&amp;#39;cases.samples&amp;#39;)) %&amp;gt;%
    results_all() %&amp;gt;%
    as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And do the same, but now looking for gene expression from tumors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tm_ge_files = files() %&amp;gt;%
    GenomicDataCommons::filter(~   cases.samples.sample_type==&amp;#39;Primary Tumor&amp;#39; &amp;amp;
               cases.project.project_id == &amp;#39;TCGA-BRCA&amp;#39; &amp;amp;
               analysis.workflow_type == &amp;quot;HTSeq - Counts&amp;quot;) %&amp;gt;%
    expand(c(&amp;#39;cases&amp;#39;,&amp;#39;cases.samples&amp;#39;)) %&amp;gt;%
    results_all() %&amp;gt;%
    as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have two data frames describing the normal- and tumor-derived TCGA-BRCA gene expression files. Note that I asked the GDC to provide &lt;code&gt;cases.case_id&lt;/code&gt; as part of the record using &lt;code&gt;select()&lt;/code&gt;. By looking for the intersection of cases between these two sets of files, we can find cases that contain files derived from both tumor and normal tissue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nl_cases = bind_rows(nl_ge_files$cases, .id=&amp;#39;file_id&amp;#39;)
tm_cases = bind_rows(tm_ge_files$cases, .id=&amp;#39;file_id&amp;#39;)
matchedcases = intersect(nl_cases$case_id,
                         tm_cases$case_id)
# how many matched cases?
length(matchedcases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 112&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now create a &lt;code&gt;data.frame&lt;/code&gt; that contains file information for only the matched samples. Note
that the names of the matched cases are the file ids.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matched_nl_files = nl_cases[nl_cases$case_id %in% matchedcases, &amp;#39;file_id&amp;#39;]
matched_tm_files = tm_cases[tm_cases$case_id %in% matchedcases, &amp;#39;file_id&amp;#39;]

matched_tn_ge_file_info = rbind(subset(nl_ge_files,file_id %in% matched_nl_files),
                                subset(tm_ge_files,file_id %in% matched_tm_files))
head(matched_tn_ge_file_info)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 17
##   type  md5sum access submitter_id file_id id    data_category experimental_st…
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;           
## 1 gene… e6771… open   327a6650-be… bb7e83… bb7e… Transcriptom… RNA-Seq         
## 2 gene… 18025… open   9570c964-8d… a03b24… a03b… Transcriptom… RNA-Seq         
## 3 gene… 69f47… open   7133f91c-41… 987bf9… 987b… Transcriptom… RNA-Seq         
## 4 gene… 57daa… open   05eb3daf-d3… 7d970a… 7d97… Transcriptom… RNA-Seq         
## 5 gene… 67e69… open   b504f2d6-33… 1e97e8… 1e97… Transcriptom… RNA-Seq         
## 6 gene… 92526… open   d46b61b0-6f… b9cc9d… b9cc… Transcriptom… RNA-Seq         
## # … with 9 more variables: data_type &amp;lt;chr&amp;gt;, state &amp;lt;chr&amp;gt;, file_name &amp;lt;chr&amp;gt;,
## #   cases &amp;lt;named list&amp;gt;, data_format &amp;lt;chr&amp;gt;, updated_datetime &amp;lt;chr&amp;gt;,
## #   created_datetime &amp;lt;chr&amp;gt;, file_size &amp;lt;int&amp;gt;, acl &amp;lt;named list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the gene expression data are not that big, we can use the GDC API to download the data files directly. The GenomicDataCommons uses a cache for files, so the first time the code
below is run, data will be downloaded. After that, if the cache is in place, the cached
files will be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fnames = lapply(as.character(matched_tn_ge_file_info$file_id),
                  function(file_id) {
                      gdcdata(file_id, progress = FALSE)
                  })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;fnames&lt;/code&gt; should be a list of file names that can be read into and processed with R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.0.0 (2020-04-24)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.3
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] dplyr_0.8.5               GenomicDataCommons_1.12.0
## [3] magrittr_1.5             
## 
## loaded via a namespace (and not attached):
##  [1] SummarizedExperiment_1.18.1 tidyselect_1.1.0           
##  [3] xfun_0.13                   purrr_0.3.4                
##  [5] lattice_0.20-41             vctrs_0.3.0                
##  [7] htmltools_0.4.0             stats4_4.0.0               
##  [9] yaml_2.2.1                  utf8_1.1.4                 
## [11] rlang_0.4.6                 pillar_1.4.4               
## [13] glue_1.4.1                  rappdirs_0.3.1             
## [15] BiocGenerics_0.34.0         matrixStats_0.56.0         
## [17] GenomeInfoDbData_1.2.3      lifecycle_0.2.0            
## [19] stringr_1.4.0               zlibbioc_1.34.0            
## [21] blogdown_0.18               evaluate_0.14              
## [23] Biobase_2.48.0              knitr_1.28                 
## [25] IRanges_2.22.1              GenomeInfoDb_1.24.0        
## [27] parallel_4.0.0              curl_4.3                   
## [29] fansi_0.4.1                 Rcpp_1.0.4.6               
## [31] readr_1.3.1                 BiocManager_1.30.10        
## [33] DelayedArray_0.14.0         S4Vectors_0.26.1           
## [35] jsonlite_1.6.1              XVector_0.28.0             
## [37] hms_0.5.3                   digest_0.6.25              
## [39] stringi_1.4.6               bookdown_0.19              
## [41] GenomicRanges_1.40.0        grid_4.0.0                 
## [43] cli_2.0.2                   tools_4.0.0                
## [45] bitops_1.0-6                RCurl_1.98-1.2             
## [47] tibble_3.0.1                crayon_1.3.4               
## [49] pkgconfig_2.0.3             ellipsis_0.3.1             
## [51] Matrix_1.2-18               xml2_1.3.2                 
## [53] assertthat_0.2.1            rmarkdown_2.1              
## [55] httr_1.4.1                  R6_2.4.1                   
## [57] compiler_4.0.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
