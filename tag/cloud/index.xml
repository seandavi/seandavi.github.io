<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cloud | Academic</title>
    <link>/tag/cloud/</link>
      <atom:link href="/tag/cloud/index.xml" rel="self" type="application/rss+xml" />
    <description>cloud</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 05 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>cloud</title>
      <link>/tag/cloud/</link>
    </image>
    
    <item>
      <title>Build and deploy an NCBI GEO metadata fetch API</title>
      <link>/post/cloud-run-notes/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/cloud-run-notes/</guid>
      <description>&lt;p&gt;In this post, I want to demonstrate building a simple web API converts
an NCBI &lt;a href=&#34;https://ncbi.nlm.nih.gov/geo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GEO&lt;/a&gt; accession and associated metadata to &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt; format, and
then deploy that application as a web service on Google Cloud Platform
&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I hear from GEOquery users that sometimes they just want to get the
metadata for one or more accessions rather than getting the entire GEO
record. My &lt;a href=&#34;https://github.com/omicidx/omicidx-parsers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omicidx-parser&lt;/a&gt; library has functionality to do this
conversion. We will leverage this functionality to build a small web
application that we can deploy using &lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; approach to stand up
an API that returns a &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt; conversion of GEO metadata.&lt;/p&gt;
&lt;h2 id=&#34;tooling&#34;&gt;Tooling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Google cloud platform, specifically &lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;python programming language&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; library for web API development&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[docker] containers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;git and github&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Code: &lt;a href=&#34;https://github.com/seandavi/blog-code/&#34;&gt;https://github.com/seandavi/blog-code/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-will-i-learn-and-do&#34;&gt;What will I learn and do?&lt;/h2&gt;
&lt;p&gt;In this post, we will learn the basics of &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; through
examples. Additionally, we will learn a bit about web app development
using the &lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; python web framework.&lt;/p&gt;
&lt;p&gt;At the end of this post, you will have a containerized web application
running locally that can take a GEO accession and return a &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/an-introduction-to-json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;json&lt;/a&gt;
version of the metadata.&lt;/p&gt;
&lt;p&gt;If you have a Google Cloud Platform account that allows you to create
projects, you should be able to run your application in Google Cloud
Run.&lt;/p&gt;
&lt;h2 id=&#34;what-is-cloud-run&#34;&gt;What is Cloud Run?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; is a fully managed compute platform that automatically
scales your stateless containers. In other words, write a web
application, place it in a &lt;a href=&#34;https://docker.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker container&lt;/a&gt;, and then deploy to the
cloud. Cloud Run is one of a family of technologies referred to as
&lt;a href=&#34;https://en.wikipedia.org/wiki/Serverless_computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serverless&lt;/a&gt; it abstracts away all infrastructure management. Cloud
Run is built upon an open standard, &lt;a href=&#34;https://knative.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;, enabling the
portability of your applications.&lt;/p&gt;
&lt;h2 id=&#34;build-your-app&#34;&gt;Build your app&lt;/h2&gt;
&lt;p&gt;Sorry, but I&amp;rsquo;m going to let the code do most of the talking here. In
short, Google Cloud Run supports any web development language or
framework that can be deployed as a docker container.&lt;/p&gt;
&lt;p&gt;Here, I will use python and adopt &lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fastapi&lt;/a&gt; a framework for building
performant REST APIs.&lt;/p&gt;
&lt;p&gt;The GEO parsing will leverage the &lt;a href=&#34;https://github.com/omicidx/omicidx-parsers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;omicidx-parser&lt;/a&gt; library.&lt;/p&gt;
&lt;h2 id=&#34;containers-are-front-and-center&#34;&gt;Containers are front-and-center&lt;/h2&gt;
&lt;p&gt;Google&amp;rsquo;s &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; creates a &amp;ldquo;service&amp;rdquo; that accepts http requests
and returns responses. The requests and responses are not predefined
by Cloud Run, allowing any approach that your chosen programming
language supports. Once your web application is written and tested
locally, create a &lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerfile&lt;/a&gt; that generates a containerized version
of it. The generated docker container is what you will pass along to
&lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; as the application. &lt;a href=&#34;https://cloud.google.com/run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt; will then create a
&amp;ldquo;service&amp;rdquo; by setting up your docker container to run &amp;ldquo;on demand&amp;rdquo; when
a web request is directed to it.&lt;/p&gt;
&lt;p&gt;Any programming dependencies, operating system or system dependencies,
or necessary software are simply included in the &lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dockerfile&lt;/a&gt;
description to be incorporated into the container. Local testing is
straightforward by simply running the docker container locally.&lt;/p&gt;
&lt;h2 id=&#34;example-application-and-deployment&#34;&gt;Example application and deployment&lt;/h2&gt;
&lt;p&gt;As of today, I am using &lt;a href=&#34;https://python-poetry.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poetry&lt;/a&gt; for python dependency management
(think pip and virtualenv replacement) and package building. To get
started with poetry, install:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py \
  | python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;ll leave the poetry parts of things out for now, but you will see
some poetry command lines to build the cloud run environment.&lt;/p&gt;
&lt;p&gt;Next, checkout the &lt;a href=&#34;https://github.com/seandavi/blog-code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example repo&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/seandavi/blog-code
cd blog-code/geo-metadata-cloud-run/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To set up a local development environment and then test the app
locally, I use a couple of poetry commands (see poetry docs for what
these do, specifically).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install
poetry run uvicorn app.main:app
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, navigate to http://localhost:8000/docs. There, by default, you are redirected to the auto-generated &lt;a href=&#34;https://swagger.io/docs/specification/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAPI&lt;/a&gt; API documentation (yes, you get this &lt;em&gt;for free&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;To see the results for a specific accession, click here: http://localhost:8000/geo/GSM48743. An example response will look like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;GSM48743&amp;quot;: {
        &amp;quot;title&amp;quot;: &amp;quot;Patient sample ST163, Liposarcoma&amp;quot;,
        &amp;quot;status&amp;quot;: &amp;quot;Public on Oct 11 2005&amp;quot;,
        &amp;quot;submission_date&amp;quot;: &amp;quot;2005-04-21&amp;quot;,
        &amp;quot;last_update_date&amp;quot;: &amp;quot;2005-11-22&amp;quot;,
        &amp;quot;type&amp;quot;: &amp;quot;RNA&amp;quot;,
        &amp;quot;anchor&amp;quot;: null,
        &amp;quot;contact&amp;quot;: {
            &amp;quot;city&amp;quot;: &amp;quot;Bethesda&amp;quot;,
            &amp;quot;name&amp;quot;: {
                &amp;quot;first&amp;quot;: &amp;quot;Sean&amp;quot;,
                &amp;quot;middle&amp;quot;: &amp;quot;&amp;quot;,
                &amp;quot;last&amp;quot;: &amp;quot;Davis&amp;quot;
            },
            &amp;quot;email&amp;quot;: &amp;quot;sdavis2@mail.nih.gov&amp;quot;,
            &amp;quot;state&amp;quot;: &amp;quot;MD&amp;quot;,
            &amp;quot;address&amp;quot;: &amp;quot;37 Convent Drive, Room 6138&amp;quot;,
            &amp;quot;department&amp;quot;: null,
            &amp;quot;country&amp;quot;: &amp;quot;USA&amp;quot;,
            &amp;quot;web_link&amp;quot;: null,
            &amp;quot;institute&amp;quot;: &amp;quot;National Cancer Institute&amp;quot;,
            &amp;quot;zip_postal_code&amp;quot;: null,
            &amp;quot;phone&amp;quot;: &amp;quot;301-435-2652&amp;quot;
        },
        &amp;quot;description&amp;quot;: &amp;quot;Diagnosis: Liposarcoma\nKeywords = sarcoma, cDNA, Liposarcoma&amp;quot;,
        &amp;quot;accession&amp;quot;: &amp;quot;GSM48743&amp;quot;,
        &amp;quot;biosample&amp;quot;: null,
        &amp;quot;tag_count&amp;quot;: null,
        &amp;quot;tag_length&amp;quot;: null,
        &amp;quot;platform_id&amp;quot;: &amp;quot;GPL1977&amp;quot;,
        &amp;quot;hyb_protocol&amp;quot;: null,
        &amp;quot;channel_count&amp;quot;: 2,
        &amp;quot;scan_protocol&amp;quot;: null,
        &amp;quot;data_row_count&amp;quot;: 12600,
        &amp;quot;library_source&amp;quot;: null,
        &amp;quot;overall_design&amp;quot;: null,
        &amp;quot;sra_experiment&amp;quot;: null,
        &amp;quot;data_processing&amp;quot;: null,
        &amp;quot;supplemental_files&amp;quot;: [
            &amp;quot;NONE&amp;quot;
        ],
        &amp;quot;channels&amp;quot;: [
            {
                &amp;quot;label&amp;quot;: null,
                &amp;quot;taxid&amp;quot;: [
                    9606
                ],
                &amp;quot;molecule&amp;quot;: &amp;quot;total RNA&amp;quot;,
                &amp;quot;organism&amp;quot;: &amp;quot;Homo sapiens&amp;quot;,
                &amp;quot;source_name&amp;quot;: &amp;quot;Liposarcoma&amp;quot;,
                &amp;quot;label_protocol&amp;quot;: null,
                &amp;quot;growth_protocol&amp;quot;: null,
                &amp;quot;extract_protocol&amp;quot;: null,
                &amp;quot;treatment_protocol&amp;quot;: null,
                &amp;quot;characteristics&amp;quot;: [
                    
                ]
            },
            {
                &amp;quot;label&amp;quot;: null,
                &amp;quot;taxid&amp;quot;: [
                    9606
                ],
                &amp;quot;molecule&amp;quot;: &amp;quot;total RNA&amp;quot;,
                &amp;quot;organism&amp;quot;: &amp;quot;Homo sapiens&amp;quot;,
                &amp;quot;source_name&amp;quot;: &amp;quot;Pooled sarcoma cell lines&amp;quot;,
                &amp;quot;label_protocol&amp;quot;: null,
                &amp;quot;growth_protocol&amp;quot;: null,
                &amp;quot;extract_protocol&amp;quot;: null,
                &amp;quot;treatment_protocol&amp;quot;: null,
                &amp;quot;characteristics&amp;quot;: [
                    
                ]
            }
        ],
        &amp;quot;contributor&amp;quot;: [
            
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the web application is running locally. We can
&amp;ldquo;containerize&amp;rdquo; the application by building the docker container. The
&lt;a href=&#34;https://github.com/seandavi/blog-code/blob/master/geo-metadata-cloud-run/Dockerfile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;associated Dockerfile&lt;/a&gt; has instructions for doing so. To actually
build the docker image, do the following, replacing &lt;code&gt;PROJECT_ID&lt;/code&gt; below with
your GCP project ID. You can view your project ID by running the
command.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud config get-value project.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PROJECT_ID=&#39;my-google-project-name&#39;
docker build -t gcr.io/${PROJECT_ID}/geo-cloud-run .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Push the docker image to the Google Cloud Project image
repository. You can also use dockerhub name/url if you like.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker push gcr.io/${PROJECT_ID}/geo-cloud-run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the app as a dockerized app locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 8000:80 gcr.io/${PROJECT_ID}/geo-cloud-run 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, you should now be able to access the API at http://localhost:8000/docs.&lt;/p&gt;
&lt;p&gt;One more step, deployment, gets us the application running in the
cloud as a serverless application. Deploy using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud run deploy --image gcr.io/${PROJECT_ID}/geo-cloud-run --platform managed
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;You will be prompted for the service name: press Enter to accept the
default name, geo-cloud-run&lt;/li&gt;
&lt;li&gt;You will be prompted for region: select the region of your choice,
for example us-central1.&lt;/li&gt;
&lt;li&gt;You will be prompted to allow unauthenticated invocations: respond &lt;code&gt;y&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then wait a few moments until the deployment is complete. On success,
the command line displays the service URL. If you navigate to that
service URL, you will be accessing your &lt;em&gt;entirely serverless&lt;/em&gt;
application. Build more complicated applications by simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adjust python code.&lt;/li&gt;
&lt;li&gt;Rebuild docker image.&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;gcloud run update ....&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When completed, simply run &lt;code&gt;gcloud run delete ....&lt;/code&gt; to remove your
service and application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experimenting with Github Actions</title>
      <link>/post/message-queues-to-orchestrate-hpc-batch-runs/</link>
      <pubDate>Fri, 31 Jan 2020 18:00:00 -0400</pubDate>
      <guid>/post/message-queues-to-orchestrate-hpc-batch-runs/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Actions&lt;/a&gt; allow flexible and potentially complicated actions
that comprise workflows that respond to &lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt; on Github. Continuous
integration, messaging Slack, greeting new contributors, deploying
applications, and many other templates are ready for customization and
integration into any repo.&lt;/p&gt;
&lt;p&gt;As of this writing, GitHub Actions are available as a &lt;em&gt;beta&lt;/em&gt; feature,
requiring &lt;a href=&#34;https://github.com/features/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;signup&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;definitionshttpshelpgithubcomenarticlesabout-github-actionscore-concepts-for-github-actions&#34;&gt;&lt;a href=&#34;https://help.github.com/en/articles/about-github-actions#core-concepts-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Definitions&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;action&#34;&gt;Action&lt;/h3&gt;
&lt;p&gt;An action is the smallest portable building block of a
workflow. Actions are combined as steps to form a workflow. Actions can be
created from scratch, modified from &lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly available actions&lt;/a&gt;, and
shared with others.&lt;/p&gt;
&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;
&lt;p&gt;A workflow is a collection of steps (actions) that describe a process
that will execute in response to GitHub &lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt;. A YAML
file that defines your workflow configuration with at least one
job. This file lives in the root of your GitHub repository in the
&lt;code&gt;.github/workflows&lt;/code&gt; directory. Multiple workflow files may exist and
all will be active.&lt;/p&gt;
&lt;h3 id=&#34;step&#34;&gt;Step&lt;/h3&gt;
&lt;p&gt;A step is a set of tasks performed by a job. Each step in a job
executes in the same virtual environment, allowing the actions in that
job to share information using the filesystem. Steps can run commands
(shell commands, etc.) or actions.&lt;/p&gt;
&lt;h3 id=&#34;virtual-environment&#34;&gt;Virtual environment&lt;/h3&gt;
&lt;p&gt;GitHub has hosts for Linux, macOS, and Windows that can serve as
execution environments for workflows.&lt;/p&gt;
&lt;h3 id=&#34;runner&#34;&gt;Runner&lt;/h3&gt;
&lt;p&gt;Jobs run on a service that waits in virtual environment that waits for available
jobs. The runner takes jobs one-at-a-time and runs each, reporting
logs, etc., back to Github.&lt;/p&gt;
&lt;h3 id=&#34;event&#34;&gt;Event&lt;/h3&gt;
&lt;p&gt;Workflows can be triggered by a [repository event] such as a push, a
new issue, etc.&lt;/p&gt;
&lt;h3 id=&#34;artifact&#34;&gt;Artifact&lt;/h3&gt;
&lt;p&gt;Artifacts are the files that are created during a workflow. These can
be deployed, used in other workflows, published, etc. Additional
actions allow working with artifacts.&lt;/p&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;h3 id=&#34;create-repo&#34;&gt;Create repo&lt;/h3&gt;
&lt;p&gt;Here, I simulate creating a simple python package by forking &lt;a href=&#34;https://github.com/bast/somepackage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
example repo&lt;/a&gt; by
&lt;a href=&#34;https://github.com/bast&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bast&lt;/a&gt;. The package is to exemplify some best
practices for structuring python code. However, here it serves as a
simple example of a formal python package that we can build and test.&lt;/p&gt;
&lt;p&gt;Login to Github, navigate to the &lt;a href=&#34;https://github.com/bast/somepackage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example repo&lt;/a&gt;, and &lt;a href=&#34;https://help.github.com/en/articles/fork-a-repo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fork the
repo&lt;/a&gt;.&lt;/p&gt;


















&lt;figure id=&#34;figure-fork-the-example-repohttpsgithubcombastsomepackage&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;fork-repo.png&#34; data-caption=&#34;Fork the &amp;lt;a href=&amp;#34;https://github.com/bast/somepackage&amp;#34;&amp;gt;example repo&amp;lt;/a&amp;gt;.&#34;&gt;


  &lt;img src=&#34;fork-repo.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fork the &lt;a href=&#34;https://github.com/bast/somepackage&#34;&gt;example repo&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After forking, a fresh package will be available in your repository.&lt;/p&gt;
&lt;h3 id=&#34;add-a-github-action&#34;&gt;Add a github action&lt;/h3&gt;
&lt;p&gt;After ensuring that you have [signed up] for Github Actions and have
verified that you are &amp;ldquo;in&amp;rdquo; by noticing the &lt;code&gt;Actions&lt;/code&gt; button at the top
of your github repository:&lt;/p&gt;


















&lt;figure id=&#34;figure-the-github-actions-button-should-be-present-in-order-to-proceed&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;action-button.png&#34; data-caption=&#34;The Github Actions button should be present in order to proceed.&#34;&gt;


  &lt;img src=&#34;action-button.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Github Actions button should be present in order to proceed.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now, we can proceed with adding a workflow from the set of recommended
workflow templates (or others). Clicking on the &lt;code&gt;Actions&lt;/code&gt; button will
bring up the following screen.&lt;/p&gt;


















&lt;figure id=&#34;figure-choose-the-python-package-workflow&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;choose-action.png&#34; data-caption=&#34;Choose the python package workflow.&#34;&gt;


  &lt;img src=&#34;choose-action.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Choose the python package workflow.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Click on the &lt;code&gt;Set up this workflow&lt;/code&gt; on the &amp;ldquo;Python package&amp;rdquo;
card. That will bring up the following screen.&lt;/p&gt;


















&lt;figure id=&#34;figure-the-workflow-editor&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;workflow-editor.png&#34; data-caption=&#34;The workflow editor.&#34;&gt;


  &lt;img src=&#34;workflow-editor.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The workflow editor.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;What is happening at this point is that Github is about to add a new
file to your repository. The file will be located at
&lt;code&gt;somepackage/.github/workflows&lt;/code&gt;. Edit the filename as you see
fit.&lt;/p&gt;
&lt;p&gt;The contents of the workflow file will look like the listing below by
default and follows &lt;a href=&#34;https://help.github.com/en/articles/workflow-syntax-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the workflow
syntax&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;name: Python package

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [2.7, 3.5, 3.6, 3.7]

    steps:
    - uses: actions/checkout@v1
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Lint with flake8
      run: |
        pip install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest
      run: |
        pip install pytest
        pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;commit-new-workflow-file&#34;&gt;Commit new workflow file&lt;/h3&gt;
&lt;p&gt;Once you have made any edits (no need to, though), go ahead and click
the &lt;code&gt;Start Commit&lt;/code&gt; button.&lt;/p&gt;


















&lt;figure id=&#34;figure-make-the-first-commit&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;first-commit.png&#34; data-caption=&#34;Make the first commit.&#34;&gt;


  &lt;img src=&#34;first-commit.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Make the first commit.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;build-results&#34;&gt;Build results&lt;/h3&gt;
&lt;p&gt;By navigating back to &lt;code&gt;Actions&lt;/code&gt;, you should see that the workflow is
running or has already completed. An example of what a running
workflow might look like is below.&lt;/p&gt;


















&lt;figure id=&#34;figure-image-credit-github-bloghttpsgithubblog2019-08-08-github-actions-now-supports-ci-cd&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.blog/wp-content/uploads/2019/08/streaming.gif?w=918&amp;amp;resize=918%2C802&#34; data-caption=&#34;Image Credit: &amp;lt;a href=&amp;#34;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&amp;#34;&amp;gt;GitHub Blog&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;https://github.blog/wp-content/uploads/2019/08/streaming.gif?w=918&amp;amp;resize=918%2C802&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Image Credit: &lt;a href=&#34;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&#34;&gt;GitHub Blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You are free to investigate the logs, etc.&lt;/p&gt;
&lt;h3 id=&#34;modifying-the-workflow&#34;&gt;Modifying the workflow&lt;/h3&gt;
&lt;p&gt;Modifying the workflow is as simple as making edits to the
&lt;code&gt;.github/workflows/...yaml&lt;/code&gt; file. For example, you might consider
changing the name of the workflow and modifying the python versions
that are used for testing.&lt;/p&gt;
&lt;h3 id=&#34;workflow-status-badge&#34;&gt;Workflow status badge&lt;/h3&gt;
&lt;p&gt;Follow &lt;a href=&#34;https://help.github.com/en/articles/configuring-a-workflow#adding-a-workflow-status-badge-to-your-repository&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these
instructions&lt;/a&gt;
to add a status badge to your repo that will be updated with each
workflow execution. Example code is here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[![Actions
Status](https://github.com/seandavi/somepackage/workflows/Python%20package/badge.svg)](https://github.com/seandavi/somepackage/actions)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;%20&lt;/code&gt; that represents a url-encoded &lt;code&gt;space&lt;/code&gt; character. The resulting badge looks like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/seandavi/somepackage/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://github.com/seandavi/somepackage/workflows/Python%20package/badge.svg&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Github actions and workflows add continuous integration and other
capabilities that are configurable via text files that are captured
and versioned alongside code. Therefore, actions can be shared,
modified, and manipulated with standard text editors, etc.&lt;/p&gt;
&lt;p&gt;Many actions are &lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly
available&lt;/a&gt;. Example workflows are also
&lt;a href=&#34;https://github.com/actions/starter-workflows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easy to find&lt;/a&gt;. For
those who are partial to &lt;em&gt;R&lt;/em&gt;, there is the R-centric
&lt;a href=&#34;https://r-lib.github.io/ghactions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ghactions&lt;/a&gt; package that 1)
provides workflow templates for common R projects (packages,
RMarkdown, …) with sensible defaults and 2) wraps and curates relevant
existing external actions, such as those to deploy to GitHub Pages or
Netlify.&lt;/p&gt;
&lt;p&gt;Hardware available as of this writing is &lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#supported-virtual-environments-and-hardware-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;described
here&lt;/a&gt;
and includes Windows, Linux, and MacOS environments. When a workflow
is running, multiple &lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#environment-variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;environment
variables&lt;/a&gt;
are set and accessible to software. Github actions allow the user to
create
&lt;a href=&#34;https://help.github.com/en/articles/virtual-environments-for-github-actions#creating-and-using-secrets-encrypted-variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;secrets&lt;/a&gt;
to securely store credentials for use inside run environments. Github
actions are container-ready, allowing &lt;a href=&#34;https://help.github.com/en/articles/creating-a-docker-container-action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;containerized
actions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post, like many of my posts, is very operational, but I have
found that a worked example is usually more valuable than long
expository posts. That said, I always appreciate comments and
suggestions for improvements (see below).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Package Registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/articles/workflow-syntax-for-github-actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workflow Syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.github.com/webhooks/#events&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repository events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publicly available actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.community/t5/GitHub-Actions/bd-p/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Community support site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Using google cloud registry for private docker images</title>
      <link>/post/using-google-cloud-registry/</link>
      <pubDate>Wed, 06 Feb 2019 13:12:54 -0500</pubDate>
      <guid>/post/using-google-cloud-registry/</guid>
      <description>


&lt;p&gt;In this post, I will quickly build a docker image containing the
sra-toolkit and a key for dbGaP downloads. Because the key file is
private, I will be using the secure Google Container Registry to store
the image for later use in genomics workflows.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Container technologies like &lt;a href=&#34;https://docker.io/&#34;&gt;docker&lt;/a&gt; enable quick and easy
encapsulation of software, dependencies, and operating systems. One or
more containers can render entire software ecosystems portable,
enhance reproducibility and reusability, and facilitate sharing of
software, tools, and even infrastructure.&lt;/p&gt;
&lt;p&gt;While &lt;a href=&#34;https://hub.docker.com/&#34;&gt;DockerHub&lt;/a&gt; is perhaps the most well-known registry where such
docker images can be housed, others such as &lt;a href=&#34;https://quay.io/&#34;&gt;quay.io&lt;/a&gt; are also
available. Commercial cloud environments, such as the &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud
Platform&lt;/a&gt; often offer their own registries that use the same secure
access controls as other cloud services, &lt;em&gt;allowing docker images with
proprietary or private information to be stored and accessed
securely&lt;/em&gt;. They are also typically quite integrated with commercial
cloud services (&lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;gcr docs&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I am experimenting with the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Google Container Registry&lt;/a&gt; (GCR) for a
bioinformatics project that I plan to perform in on Google Cloud. This
blog post simply serves as notes to myself about details of using that
system.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;A Google Cloud &lt;a href=&#34;https://cloud.google.com/container-registry/docs/quickstart&#34;&gt;account and project&lt;/a&gt; is required to follow along here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/sdk/docs/&#34;&gt;Install the Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/&#34;&gt;Install
Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to allow docker to use google for authorization, we need to
do this one-time command to get tie docker to google.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;gcloud auth configure-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Answer “yes” to the prompt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-docker-image-and-adding-to-gcr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building docker image and adding to GCR&lt;/h2&gt;
&lt;p&gt;We are going to build a simple docker file that includes the
&lt;a href=&#34;https://github.com/ncbi/sra-tools&#34;&gt;sra-tools&lt;/a&gt; package, import a private key for downloading files that
are protected, and then store that image to GCR.&lt;/p&gt;
&lt;p&gt;The Dockerfile is given below. Note that I &lt;strong&gt;am not including the dbGaP
key file, as it is private&lt;/strong&gt;, but you could modify the Dockerfile to
include your own key file(s) or simply remove the dbGaP access details
entirely.&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/seandavi/410cfbbed6b8db0b4928eb36236e7dda.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Building the container is one line:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker build -t sratoolkit .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the docker image has been created. Run it locally to
test, for example.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -ti sratoolkit /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the docker container, all the sra-toolkit binaries are
available.&lt;/p&gt;
&lt;p&gt;Each Google Cloud Project has a private GCR. Therefore, GCR urls
include the &lt;code&gt;PROJECT-ID&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker tag sratoolkit gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2
docker push gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;For fun, we can use the image and the &lt;code&gt;fastq-dump&lt;/code&gt; utility to download
a single SRA run. The docker image will only run as long as necessary
to perform the fastq dump and then will terminate. This post is not
about the details of running docker, but note that the following will
result in the fastq file from the &lt;code&gt;fastq-dump&lt;/code&gt; command ending up in
the &lt;code&gt;/tmp&lt;/code&gt; directory on your machine.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -v /tmp:/data -ti  gcr.io/isb-cgc-01-0006/sratoolkit:2.9.2\
    /bin/bash -c &amp;quot;cd /root/ncbi/dbGaP-16049/ \
    &amp;amp;&amp;amp; fastq-dump   --split-files --gzip \
    --skip-technical -X 10000   SRR390728&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will result in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Read 10000 spots for SRR390728
Written 10000 spots for SRR390728&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;best-practices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Best practices&lt;/h2&gt;
&lt;p&gt;Storing keys, key files, and any other private information in a docker
image is a risky operation and is not a best practice. Leveraging the
security model of GCR mitigates these issues. However, it is &lt;strong&gt;really
easy&lt;/strong&gt; to forget about the information that might be leaked if this
image were shared or pushed to a public repository like DockerHub.&lt;/p&gt;
&lt;p&gt;For a discussion of other models for accessing private information
inside a docker container, see &lt;a href=&#34;https://www.ctl.io/developers/blog/post/tutorial-protecting-sensitive-info-docker&#34;&gt;this blog post, for example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From here&lt;/h2&gt;
&lt;p&gt;At this point, we have created an environment for building docker
images, stored a proprietary docker image in the Google Cloud
Registry, and successfully used the image to run a simple command to
perform a &lt;code&gt;fastq-dump&lt;/code&gt; with the files ending up on the docker host
machine. This image can be used anywhere a docker image can run, but
only if google authentication &lt;a href=&#34;#preliminaries&#34;&gt;has been tied into docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/genomics/docs/quickstart&#34;&gt;Google Genomics Pipeline API&lt;/a&gt; is built around scalable genomic
workflows and uses docker images for workflow tasks. The image here
can be used for dbGaP genomic data extraction as part of such a
workflow, all within the secure environment of google cloud.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The cancer data ecosystem: data and cloud resources for cancer genomic data science</title>
      <link>/talk/2018-10-16-cancer-data-ecosystem-cmu/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 -0400</pubDate>
      <guid>/talk/2018-10-16-cancer-data-ecosystem-cmu/</guid>
      <description>&lt;div class=&#34;responsive-wrap&#34;&gt;
  &lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSksBmgOBEjIdxxATW3fWG-EZoULxDc-tg94y8QdNdKa6NPnDWI6el8yHRSrXDLr09wDRNy4n-xeEqR/embed?start=true&amp;amp;loop=true&amp;amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Infrastructure-as-Code: Building the Bioconductor Conference AMI With Packer</title>
      <link>/post/2018-07-19-infrastructure-as-code-building-the-bioconductor-conference-ami-with-packer/</link>
      <pubDate>Thu, 19 Jul 2018 14:35:47 -0400</pubDate>
      <guid>/post/2018-07-19-infrastructure-as-code-building-the-bioconductor-conference-ami-with-packer/</guid>
      <description>


&lt;p&gt;One of the main features of the annual &lt;a href=&#34;https://bioc2018.bioconductor.org&#34;&gt;Bioconductor Conference&lt;/a&gt; is the
proportion of time spent working with code in the form of &lt;a href=&#34;https://github.com/bioconductor/BiocWorkshops&#34;&gt;workshops&lt;/a&gt;.
To support these workshops, we ask workshop presenters to supply &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;Rmarkdown&lt;/a&gt;
materials which we collate into workshop materials. Using literate programming
approaches like Rmarkdown ensures that the workflows are self-consistent
and work as expected.&lt;/p&gt;
&lt;p&gt;In addition to the Rmarkdown workshop materials, we also need a consistent
computing environment that can support reasonably large computation, provide
high-performance network and file system access, and is essentially unlimited
in scale (we expect to have &amp;gt;150 participants, each with his/her own machine).
To do so, we use &lt;a href=&#34;https://aws.amazon.com/ec2&#34;&gt;Amazon Web Services EC2&lt;/a&gt;. The EC2 system allows us to prepare
a &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html&#34;&gt;Amazon machine “image”&lt;/a&gt;, or AMI, that contains the operating system, libraries,
the newest version of R, and all packages needed for the workshops. In the past,
creating the “image” was a manual process. This year, thanks to the work
of the workshop organizers, we had a single DESCRIPTION file that contained
all the necessary packages, allowing us to automate the process of building
and keeping updated the AMI that would be used by all participants.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.packer.io/&#34;&gt;Packer project&lt;/a&gt; is an open source tool for creating identical
machine images for multiple platforms from a single source
configuration. Packer is lightweight, runs on every major operating
system, and is highly performant, creating machine images for multiple
platforms in parallel. In this context, a machine image is a single
static unit that contains a pre-configured operating system and
installed software which is used to quickly create new running
machines. Machine image formats change for each platform. Some
examples include AMIs for EC2, VMDK/VMX files for VMware, OVF exports
for VirtualBox, etc.&lt;/p&gt;
&lt;p&gt;Biocoductor is cloud-ready and maintains &lt;a href=&#34;https://bioconductor.org/help/bioconductor-cloud-ami/&#34;&gt;basic AMIs for Bioconductor&lt;/a&gt;. Rather
than needing to start with a generic Linux AMI as the “base” for our
Bioconductor conference AMI, I will use
the most recent &lt;a href=&#34;https://bioconductor.org/help/bioconductor-cloud-ami/#ami_ids&#34;&gt;Bioc-devel AMI&lt;/a&gt; as the base. Packer uses a &lt;a href=&#34;https://www.packer.io/intro/getting-started/build-image.html#the-template&#34;&gt;json format file&lt;/a&gt;
to describe, &lt;em&gt;in code&lt;/em&gt;, the AMI that we want to build. The file for building the image
is listed below in its entirety. The current version of the packer json file
is available in this &lt;a href=&#34;https://github.com/seandavi/terraform-can/tree/master/packer&#34;&gt;github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To build the image, first &lt;a href=&#34;https://www.packer.io/docs/builders/amazon.html#authentication&#34;&gt;set up AWS authentication&lt;/a&gt; as outlined on the
packer website. If you do not have an AWS account, you will not be able
to actually build the AMI. Next, &lt;a href=&#34;https://www.packer.io/intro/getting-started/install.html&#34;&gt;install packer&lt;/a&gt; and ensure that it is in the path.
Finally, save the file below as, for example, &lt;code&gt;bioc_2018.json&lt;/code&gt;. In the
directory containing the json file, execute packer:&lt;/p&gt;
&lt;pre class=&#34;sh&#34;&gt;&lt;code&gt;packer build bioc_2018.json&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This build takes quite some time (perhaps 20 minutes or so).&lt;/p&gt;
&lt;p&gt;In terms of details, briefly, the &lt;code&gt;instance_type&lt;/code&gt;
below was chosen to allow multicore installation using 16 threads. AWS [spot pricing]
is used to minimize costs (see &lt;code&gt;spot_pricing&lt;/code&gt; and &lt;code&gt;spot_pricing_auto_product&lt;/code&gt; below).
Adding the &lt;code&gt;ami_groups&lt;/code&gt; set to &lt;code&gt;all&lt;/code&gt; will enable public access to the AMI. The &lt;code&gt;source_ami_filter&lt;/code&gt;
section below chooses the “base” image. In this case, I used the AMI &lt;code&gt;name&lt;/code&gt; and
specified that the AMI was “owned” by the Bioconductor organization (&lt;code&gt;&amp;quot;owners&amp;quot;: [&amp;quot;555219204010&amp;quot;]&lt;/code&gt;).
I set the disk size to 128GB of SSD storage in the &lt;code&gt;launch_block_device_mappings&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The real work is done in the &lt;code&gt;provisioners&lt;/code&gt; block. In this case, the provisioner
block specifies just two shell commands that install the necessary packages. Note
that the installation of the “Bioconductor/Biocworkshops” github package will
install all packages in the &lt;a href=&#34;https://github.com/Bioconductor/BiocWorkshops/blob/master/DESCRIPTION&#34;&gt;DESCRIPTION&lt;/a&gt; file. The final line of the packer
output will list the AMI ID that can be shared with others (since we made it public).
The AMI may take a few minutes to become fully public.&lt;/p&gt;
&lt;pre class=&#34;js&#34;&gt;&lt;code&gt;{
    &amp;quot;variables&amp;quot;: {
        &amp;quot;profile&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;region&amp;quot;:  &amp;quot;us-east-1&amp;quot;
    },
    &amp;quot;builders&amp;quot;: [
        {
            &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;,
            &amp;quot;ami_name&amp;quot;: &amp;quot;Bioconductor_Conference_2018-{{timestamp}}&amp;quot;,
            &amp;quot;instance_type&amp;quot;: &amp;quot;c5.4xlarge&amp;quot;,
            &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;,
            &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;,
            &amp;quot;source_ami_filter&amp;quot;: {
                &amp;quot;filters&amp;quot;: {
                    &amp;quot;virtualization-type&amp;quot;: &amp;quot;hvm&amp;quot;,
                    &amp;quot;name&amp;quot;: &amp;quot;Bioc 3.8 R 3.5.1&amp;quot;,
                    &amp;quot;root-device-type&amp;quot;: &amp;quot;ebs&amp;quot;
                },
                &amp;quot;owners&amp;quot;: [&amp;quot;555219204010&amp;quot;],
                &amp;quot;most_recent&amp;quot;: true
            },
            &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;,
            &amp;quot;spot_price&amp;quot;: &amp;quot;auto&amp;quot;,
            &amp;quot;spot_price_auto_product&amp;quot;: &amp;quot;Linux/UNIX&amp;quot;,
            &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;,
            &amp;quot;ami_groups&amp;quot;: [&amp;quot;all&amp;quot;],
            &amp;quot;launch_block_device_mappings&amp;quot;: [
                {
                    &amp;quot;device_name&amp;quot;: &amp;quot;/dev/sda1&amp;quot;,
                    &amp;quot;volume_size&amp;quot;: 128,
                    &amp;quot;volume_type&amp;quot;: &amp;quot;gp2&amp;quot;,
                    &amp;quot;delete_on_termination&amp;quot;: true
                }
            ]
        }
    ],
    &amp;quot;provisioners&amp;quot;: [
        {
            &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
            &amp;quot;inline&amp;quot;:[
                &amp;quot;Rscript -e &amp;#39;BiocManager::install(\&amp;quot;remotes\&amp;quot;)&amp;#39;&amp;quot;,
                &amp;quot;Rscript -e &amp;#39;options(Ncpus=16); BiocManager::install(\&amp;quot;Bioconductor/BiocWorkshops\&amp;quot;)&amp;#39;&amp;quot;,
            ]
        }
    ]
}
    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By maintaining the AMI description as code, we can ensure that the AMI is
fully reproducible (no manual installations, etc.) and, therefore, highly
reproducible and even reusable.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/seandavi/terraform-can/tree/master/packer&#34;&gt;The current version of the packer file is available on github&lt;/a&gt;. Thanks to Levi
Waldron, Lori Shepherd, Marcel Ramos, Martin Morgan, and multiple workshop
authors for their contributions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Protect Against Secrets in Git Repositories</title>
      <link>/post/protect-against-secrets-in-git-repositories/</link>
      <pubDate>Sat, 02 Dec 2017 07:50:15 -0500</pubDate>
      <guid>/post/protect-against-secrets-in-git-repositories/</guid>
      <description>&lt;p&gt;I made a mistake and am going to share it here. Please be gentle when
judging me. As penance, I spent some time to learn how to
systematically avoid making the same mistake and share that solution
here.&lt;/p&gt;
&lt;h2 id=&#34;the-prelude&#34;&gt;The prelude&lt;/h2&gt;
&lt;p&gt;I had been working on some code that I thought was
going to be throw-away example code for loading a large dataset into
&lt;a href=&#34;https://elastic.co/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ElasticSearch&lt;/a&gt;. That said, I have been saved often enough by using a
version control system (now, &lt;em&gt;always&lt;/em&gt; &lt;a href=&#34;https://git-scm.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git&lt;/a&gt;), that I use it all the
time. &amp;ldquo;Knowing&amp;rdquo; that this code would never be shared, I was careless
and included my [Amazon Web Service (AWS)] [keys] in the code while I sorted out whether
[logstash] would pick up the keys from a central config file. At some
point, I committed the file that included the keys to git. As these
things go, several days passed and I found that I had a useful project
worthy of a push to github. No keys
present in the code, etc.&amp;ndash;I checked.&lt;/p&gt;
&lt;p&gt;Within minutes (or maybe it was an hour&amp;ndash;not sure) of when I pushed the code
to github, I got an email from &lt;a href=&#34;https://aws.amazon.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS&lt;/a&gt; that, paraphrased, read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;We have
found your AWS keys in a github repository. Please fix
the problem and &lt;strong&gt;DON&amp;rsquo;T DO THAT AGAIN&lt;/strong&gt;.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Almost immediately after, I
got another email from AWS thanking me for deleting my keys. AWS
apparently scans GitHub repositories for AWS keys and has a system (I
suspect somewhat automated) for removing the exposed keys. I deleted
the GitHub repo and did some local checking and, in
retrospect, realized my mistake. Git had dutifully saved the entire
history of my project including a version of one file with AWS keys in
it. Upon pushing to GitHub, the keys were there in the history. I
breathed a sigh of relief that no harm had been done.&lt;/p&gt;
&lt;p&gt;Thankfully, I use AWS often. I logged in the next day and found that I
had about 100 servers running, in many different regions even, that I
hadn&amp;rsquo;t started. In the short period of time that the keys had been
exposed, someone had been able to create two ssh keypairs and started
the machines. Needless to say, I spent quite a bit of time working to
clean up any instances and to remove keys, etc. that could have been
exposed (all of them, of course). The speed with which someone was
able to capitalize on my mistake was simply astonishing to me. That
said, I do want to hand it to the AWS folks who take a proactive role
in protecting my account security.&lt;/p&gt;
&lt;h2 id=&#34;dont-do-that-again&#34;&gt;Don&amp;rsquo;t do that again&lt;/h2&gt;
&lt;p&gt;While I probably will not forget to check my code for &amp;ldquo;secrets&amp;rdquo; again
anytime soon, I started to look into ways to systematically check and
then preempt such occurrences. Ironically (or not), the folks at &lt;a href=&#34;https://github.com/awslabs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS
Labs&lt;/a&gt; have created a nice little project called &lt;a href=&#34;https://github.com/awslabs/git-secrets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git-secrets&lt;/a&gt; that
purports to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prevent you from committing secrets and credentials into git repositories&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And in more detail:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;git-secrets&lt;/code&gt; scans commits, commit messages, and &lt;code&gt;--no-ff&lt;/code&gt; merges
to prevent adding secrets into your git repositories. If a commit,
commit message, or any commit in a &lt;code&gt;--no-ff&lt;/code&gt; merge history matches
one of your configured prohibited regular expression patterns, then
the commit is rejected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, &lt;em&gt;prevent&lt;/em&gt; keys and secrets from &lt;em&gt;ever&lt;/em&gt; entering the git history.&lt;/p&gt;
&lt;h2 id=&#34;example-walkthrough-with-git-secrets&#34;&gt;Example walkthrough with git-secrets&lt;/h2&gt;
&lt;p&gt;In this section, I just document for myself what it takes to get and
use &lt;code&gt;git-secrets&lt;/code&gt; to protect a repository from inadvertently
committing keys to a git repository.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/git-secrets#installing-git-secrets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Installation instructions&lt;/a&gt; include both simple &lt;code&gt;Makefile&lt;/code&gt;-based installation or using &lt;a href=&#34;https://brew.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homebrew&lt;/a&gt; on my Mac.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;brew install git-secrets
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the &lt;code&gt;git-secrets&lt;/code&gt; executable should be in the
&lt;code&gt;PATH&lt;/code&gt;. In typical git style, one can use either &lt;code&gt;git-secrets&lt;/code&gt; or &lt;code&gt;git secrets&lt;/code&gt; to access functionality.&lt;/p&gt;
&lt;p&gt;To play, I create a simple git repo and &amp;ldquo;install&amp;rdquo; the &lt;code&gt;git-secrets&lt;/code&gt;
&lt;a href=&#34;https://git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git hooks&lt;/a&gt;. Before any commit can succeed, these hook
scripts must exit successfully.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# create an example git repo
mkdir secrets_example
cd secrets_example
git init
# now &amp;quot;install&amp;quot; the git-secrets hook
git secrets --install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Installed commit-msg hook to .git/hooks/commit-msg
Installed pre-commmit hook to .git/hooks/pre-commit
Installed prepare-commit-msg hook to .git/hooks/prepare-commit-msg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, the &lt;code&gt;secrets_example&lt;/code&gt; git repository has been created
and the &lt;code&gt;git-secrets&lt;/code&gt; pre-commit hook added. However, &lt;code&gt;git-secrets&lt;/code&gt;
needs to be told about what secrets to look for. We can check what
&lt;code&gt;git-secrets&lt;/code&gt; thinks is a secret.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git secrets --list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will return &amp;ldquo;nothing&amp;rdquo; at this poing. In my case, I want to
have &lt;code&gt;git-secrets&lt;/code&gt; check for AWS keys. &lt;code&gt;git-secrets&lt;/code&gt; has a special
function for doing just that&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git secrets --register-aws
git secrets --list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now the result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;secrets.providers git secrets --aws-provider
secrets.patterns [A-Z0-9]{20}
secrets.patterns (&amp;quot;|&#39;)?(AWS|aws|Aws)?_?(SECRET|secret|Secret)?_?(ACCESS|access|Access)?_?(KEY|key|Key)(&amp;quot;|&#39;)?\s*(:|=&amp;gt;|=)\s*(&amp;quot;|&#39;)?[A-Za-z0-9/\+=]{40}(&amp;quot;|&#39;)?
secrets.patterns (&amp;quot;|&#39;)?(AWS|aws|Aws)?_?(ACCOUNT|account|Account)_?(ID|id|Id)?(&amp;quot;|&#39;)?\s*(:|=&amp;gt;|=)\s*(&amp;quot;|&#39;)?[0-9]{4}\-?[0-9]{4}\-?[0-9]{4}(&amp;quot;|&#39;)?
secrets.allowed AKIAIOSFODNN7EXAMPLE
secrets.allowed wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;git-secrets&lt;/code&gt; has added a set of patterns (regular expressions) that
will be matched against text before a commit can succeed. Adding a
specific pattern for another password is also straightford.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git secrets --add &#39;MyPASSWORD[0-9]+&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Trying to commit a file with &lt;code&gt;MyPASSWORD123&lt;/code&gt; fails.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;MyPASSWORD123&#39; &amp;gt;&amp;gt; test.file
git add test.file
git commit -m &#39;test file with password&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the output&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;test.file:1:MyPASSWORD123

[ERROR] Matched one or more prohibited patterns

Possible mitigations:
- Mark false positives as allowed using: git config --add secrets.allowed ...
- Mark false positives as allowed by adding regular expressions to .gitallowed at repository&#39;s root directory
- List your configured patterns: git config --get-all secrets.patterns
- List your configured allowed patterns: git config --get-all secrets.allowed
- List your configured allowed patterns in .gitallowed at repository&#39;s root directory
- Use --no-verify if this is a one-time false positive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;git-secrets&lt;/code&gt; has a number of other &lt;a href=&#34;https://github.com/awslabs/git-secrets#options&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;features and functions&lt;/a&gt;, but the
walkthrough above is enought to get me started.&lt;/p&gt;
&lt;h2 id=&#34;additional-links&#34;&gt;Additional links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://console.aws.amazon.com/cloudwatch/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloudwatch alarms&lt;/a&gt; can be set to alert you about spending above a
threshold.&lt;/li&gt;
&lt;li&gt;The ironically-named &lt;a href=&#34;https://github.com/michenriksen/gitrob&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gitrob tool&lt;/a&gt; can scan existing GitHub repositories&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS access keys best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow the principle of &lt;a href=&#34;http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;least privileges&lt;/a&gt; on cloud accounts&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/google-kubernetes-autoscale-with-preemptible-instances/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/google-kubernetes-autoscale-with-preemptible-instances/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export POOL_NAME=&#39;preempt-1&#39;
export CLUSTER_NAME=&#39;cluster-1&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud beta container node-pools create ${POOL_NAME} --preemptible \
	   --cluster ${CLUSTER_NAME} --enable-autoscaling \
	   --min-nodes=0 --max-nodes=50 \
	   --machine-type=n1-standard-2                                    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Job
spec:
  nodeSelector:
    cloud.google.com/gke-preemptible: &amp;quot;true&amp;quot;
  template:
    spec:
      containers:
      - name: pyversion
        image: python:3.7
        command: [&amp;quot;python&amp;quot;,  &amp;quot;--version&amp;quot;]
      restartPolicy: Never
  backoffLimit: 4
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
